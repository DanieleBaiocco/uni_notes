
1. A Fixed-Window Neural Language Model, con analisi dei vantaggi su n-grams e con le sue limitazioni (pacco di slides 8)
2. RNN for text generation: da un audio si riesce a capire le parole  (pacco di slides 8)
3. Gated Recurrent Units (GRU) che ha update gate e reset gate (pacco di slides 8)
4. Character-Level Word Embeddings, in cui tramite Bi-LSTM si imparano meglio gli embeddings dei caratteri (pacco di slides 8)
5. ConvNets, con 1D convolution (pacco di slides 9)
6. Character-Aware Neural Language Models (pacco di slides 9)
7. Contextual Word Representations (pacco di slides 9)