Un'appunto prima che inizi: la DEONTOLOGIA e' la stessa cosa dell'etica KANTIANA (del pensiero KANTIANO)
# Kant
L'UTILITARISMO dice che la moralita' delle scelte dipende da quanto e' il **ritorno** in termini di well-being. Quindi **IN BASE AL** ritorno, all'*outcome*, si puo' giudicare se un'azione era MORALE O NO (e' un giudizio a posteriori).
La DEONTOLOGIA a differenza dell'UTILITARISMO, dice che **certe azioni sono BUONE O CATTIVE** **INDIPENDENTEMENTE** dall'outcome. Quindi magari **uccidere** e' sempre SBAGLIATO, indipendentemente dal fatto che magari poi l'uccisore abbia salvato delle vite con quell'azione e che**NON VENGA SCOPERTO**.Nota che nell'esempio appena fatto dell'uccisore, un UTILITARIANO direbbe che e' morale l'azione di uccidere compiuta dall'uccisore, perche' ha portato a un bene maggiore, nonostante il lato negativo di aver ucciso una persona.

KANT esprime questo concetto qua sopra, insieme a tanti altri, quando descrive cosa e' morale e cosa non lo e'.
Lui dice che "un'azione e' moralmente accettata SSE il _maxim_ dell'azione e' universabilizzabile".Universabilizzabile significa che "la gente vorrebbe che il _maxim_ su cui ho agito diventasse una legge universale"
Il _maxim_ e' spiegato in *The Kantian Perspective* in *READINGS*. 
Per vedere se e' universabilizzabile c'e' bisogno che superi un TEST. Il TEST e' questo:
1. Formula il tuo _maxim_ chiaramente, dicendo cosa vuoi fare e perche' vuoi farlo
2. Immagina un mondo in cui TUTTI supportano e compiono azioni basandosi su questo _maxim_
3. Poi chiediti: il GOAL della mia azione iniziale PUO' ESSERE RAGGIUNTO in tale mondo?
Questo processo assicura **giustizia**: sarebbe immorale copiare agli esami universitari per avere un voto piu' alto (immagina una societa' in cui tutti lo facessero,  il mio goal di avere un voto piu alto non potrebbe andare in porto perche' i professori NON si fiderebbero piu' degli studenti e non sarebbe piu' possibile copiare ).
Onesto non mi dispiacerebbe una AI che ragionasse secondo questo pensiero.

Kant ha identificato poi una distizione tra *hypothetical imperatives* e *categorical imperatives*.
I **hypothetical imperatives** sono praticamente azioni che mi permettono di arrivare a qualcosa che mi porta a raggiungere il mio **goal**. Quindi sono legati ai miei interessi, e a volte la realizzazione di un proprio goal puo' portare a agire non secondo la morale (almeno non come la morale come la intende Kant).
Questi  **hypothetical imperatives** dipendono da COSA VOGLIO in quel momento, e quindi possono cambiare. 
Il **categorical imperative** e' invece un *imperativo morale* che si applica a tutti gli umani, indipendentemente dai loro desideri e dai loro bisogno. C'e' da obbedire a questo imperativo morale, anche se non vogliamo. E anche se seguire qeusto imperativo **non** ci permette di raggiungere i nostri goals.

Quindi la moralita' di un'azione dipende solo dalla motivazione dietro quell'azione (quindi dalla parte legata alla motivazione del _maxim_). Se l'azione e' motivata da *buona volonta'* , ovvero che rispecchia il *categorical imperative* allora l'azione e' morale.
Un altro modo di vedere il *categorical imperative* e' questo:
*agire considerando gli altri come fini e non come mezzi*. Cio' non significa che NON posso mai usare qualcuno come mezzo, ma che NON DEVE ESSERE SOLO MEZZO, deve essere soprattutto un fine.

Secondo KANT inoltre, gli esseri razionali (quali gli umani), sono degni di **dignita'**, che li rende preziosi. Devono quindi essere trattati con rispetto. Ho quindi che AI systems dovrebbero rispettare gli umani.
Sono dotati di dignita' in quanto dotati di **ragione** e  **autonomi** (possono scegliere cosa fare, e in particolare di seguire la categorical imperative invece delle loro preferenze/ dei loro interessi).
Secondo KANT le cose o hanno un prezzo o hanno dignita'. Se qualcosa ha un prezzo e' possibile trovare qualcos'altro che la possa sostituire. Per gli umani questa cosa NON VALE, sono insostituibili, NON HANNO PREZZO. Di conseguenza hanno dignita'. E se i sistemi di AI avessero dignita' e autonomia? Problema zio.

Per KANT per essere razionale devo essere morale. Guarda *Argument for the Irrationality of Immorality* in *READINGS* nella parte riferita a KANT.
A volte il principio di univeralita' non per forza pero' mi da' outcomes morali (cioe' ci sono delle eccezioni che si possono trovare), quindi si anche l'idea di KANT non e' perfetta nel definire la moralita'.

# Approcci all'universalisability
Ci sono state altre visioni e altri approcci nel corso dei tempi da altri filosofi a questo concetto di cio' che e' universalmente giusto/corretto/morale
1. Richard Hare dice che i *giudizi morali* sono universalizzabile: il giudizio che una determinata azione sia giusta/sbagliata mi porta a dire che tutte le azioni a essa simili siano sbagliate. Moral judgments are universalizable in the sense that they take into account the satisfaction of everybody’s preferences (back to utilitarianism)
2. Christine Korsgaard dice che l'*umanita'*, ovvero la capacita' di riflettere e agire partendo da dei ragionamenti) ha un **valore**. Di conseguenza devo considerare l'*umanita'* di tutti gli altri allo stesso modo e rispettarla.

Se avessi dei KANTIAN ROBOTS mi piacerebbe sai, sarebbero **consistenti**, quindi **coerenti**. Adotterebbero una moralita' che in larga parte condivido. Purtroppo spesso potrebbero operare secondo una morale che non sempre e' GIUSTA (pensa alle eccezioni per cui avere un _maxim_ universalizzabile NON porta a un comportamento morale, sono poche ma esistono).

# David Ross - prima facie duties
E' un filosofo, lui ha pensato che  "Sappiamo cosa e' giusto e sbagliato. Ma ci sono diverse cose che sono importanti, indipendenti tra loro. Una di queste e' 
1. la FEDELTA': essere onesti con gli altri
2. la RIPARAZIONE: riparare nel caso abbiamo fatto del male a qualcuno
3. la GRADITUDINE: dovremmo essere grati agli altri quando ci fanno favori o ci aiutano e dovremmo ritornare questi favori.
4. NON-INJURY: non dovremmo in alcun modo ferire un altro causando infortuni sia fisici che psicologici.
5. BENEFICENCE: migliorare la vita degli altri, la loro salute, il loro benessere, ecc...
6. SELF-IMPROVEMENT dovremmo migliorare la nostra salute, felicita', benessere, sicurezza, ecc...
7. GIUSTIZIA: dovremmo cercare di distribuire le cose in maniera uguale
Questi valori possono entrare in conflitto: se segup il SELF-IMPROVEMENT magari potrei violare non so la BENEFICIENCE.
Quando vanno in conflitto, seguo quel valore che piu' sento rilevante.

# Nietzche
E' andato contro l'etica. Pensa che il superuomo, e' oltre le visioni tradizionali di buono e cattivo, oltre la moralita'. Il superuomo non trova i valori, ma DETERMINA i valori. Un Nietzche robot non piacerebbe al Prof.


# CONTRACTARISM
E' un' altra corrente di pensiero.
Il concetto dietro e' che *le azioni sono moralmente giuste perche' sono permesse da regole che persone razionali, libere e uguali hanno DECISO DI ACCETTARE* (parla di noi) ,* SOLO a condizione che GLI ALTRI LE RISPETTINO A LORO VOLTA*. Cioe' quando ACCETTIAMO il contratto sociale (che sarebbe l'insieme di leggi e norme che dice cosa e' morale e cosa non lo e') lo facciamo fidandoci del fatto che anche gli altri lo rispetteranno.

Hobs dice che spesso ci sono situazioni di TUTTI CONTRO TUTTI in natura (Homo Homini Lupus).
L'uomo tende a COOPERARE o a essere INDIVIDUALE e EGOISTA nelle scelte?
Hobbs dice che OGNUNO e' EGOISTA in natura.
Questa strategia e' adottata nel PRISIONER DILEMMA, in cui la scelta EGOISTA e' identificata come *RATIONAL BEHAVIOUR* (comportamento razionale). Se si usa questa strategia da parte di entrambi i giocatori, allora si ha IL PRISIONER DILEMMA, in cui si ha un EQUILIBRIO, ma non ritorna il massimo PAYOFF ritornabile (lo avrei avuto se entrambi avrebbero cooperato).
Con un CONTRATTO magari le cose andrebbero meglio. Con **REPEATED INTERACTIONS** come hai gia visto in COMPLEX SYSTEMS, allora posso arrivare alla soluzione che entrambi COOPERINO.
Se non ho cooperazione alcuna allroa posso andare a cambiare i **PESI DELLA PAYOFF** MATRIX (questo lo fa lo stato), in modo magari da SANZIONARE molto di  piu' (con multe e galera) chi non si comporta in maniera morale (chi non si comporta come dovrebbe). Questo e' l'approccio che Hobbs utilizzerebbe, LA PUNIZIONE, perche' per lui la natura umana e' qualcosa che se non regolato tende a DEFECTARE.

Ad ogni modo stiamo divagando, tornando a noi.
Non abbiamo ancora visto un modo per **ASSICURARCI** che il contratto sociale proposto sia **giusto**.
Come fare?
## John Rawls
Lui scrive che per avere un contratto sociale giusto allora le persone dovrebbero sceglierlo sotto un *velo di ignoranza*, quindi senza pensare al loro sesso, alla loro classe sociale, ai loro interessi, al loro grado di salute, alla loro origine.
Ha stilato 2 principi su cui basare questa societa':
1. PRIMO: Ogni persona ha lo stesso schema di liberta' di base che sia compatibile con gli altri schemi di liberta' degli altri
2. SECONDO:  le ineguaglianze sociali e economiche devono essere tale che:
	1. devono essere legate a uffici o posizioni aperti  a quelle persone che hanno le stesse condizioni di **equalita' di opportunita'**, quindi se 100 persone hanno studiato informatica, e' giusto che il posto di lavoro da informatico sia aperto a tutte queste 100 persone. E magari e' anche giusto che non sia aperto a chi ha studiato per fare il cameriere.
	2. devono essere di grande beneficio per le persone che sono **meno avvantaggiate**
	LA AI nella societa' ha peggiorato la situazione, la nostra societa' e' ancora piu' iniqua. Ha dato ancora piu' potere a chi gia' ne aveva.
## Juergen Habermas
Ha scritto *Discourse Ethics*. In questo testo scrive che:
* Una scelta o una regola e' giustificabile, e quindi valida, solo se quelli affetti da questa regola la accettano dopo che questa sia stata spiegata in un discorso ragionato e ragionevole.
* Una **norma** e' valida se le sue (prevedibili) conseguenze  e effetti collaterali causati dall'osservazione e applicazione di tale norma, siano accettati da tutti, in modo libero (senza che un tiranno stia li' a obbligarti di accettare la norma)
* The valid norms are those that would be the accepted outcome of an ” ideal speech situation”, in which all participants would be motivated solely by the desire to obtain a rational consensus and would evaluate each other’s assertions solely on the basis of reason and evidence, being free of any physical and psychological coercion
* This approach assumes that people are able to engage in discourse and converge on the recognition of reasons for norms and choices
# Virtue ethics
Immagino sia un'ulteriore corrente di pensiero per cui si dice che:
* L'etica non deve concentrarsi ne' sulle norme ne' sulle conseguenze. Un'azione e' moralmente corretta **perche' e' l'azione che una persona virtuosa farebbe in quella data situazione**.
* Visto che esistono diverse **VIRTU'**, l'azione giusta e'  quella risultante dal mix di virtu' rilevanti in quel dato contesto. Un elenco di virtu' e' : honesty; loyalty; courage; impartiality, wisdom, fidelity, generosity, compassion
* In questo pensiero si dice che l'etica non puo' esssere imparata attraverso un insieme di regole ma che serve applicarla in vari contesti e e' necessaria saggezza per fare cio'.