Quando faccio una scelta ci sono due cose che mi possono muovere:
1. **interesse personale**: quando faccio una determinata scelta in quanto mi portera' a un giovamento per un particolare interesse che ho io
2. **moralita'/etica**: posso fare un'azione seguendo dei principi etici e morali, indipendentemente dal fatto che quell'azione mi portera' un giovamento o meno dei miei interessi personali
Si hanno due tipi di moralita':
1. **Conventional (positive) morality**: coincide con le regole morali che sono accettate e fanno parte di una determinata societa'.
2. **Critical morality**: e' quella che mi interessa, e' quella indipendente dalla societa' in cui si e', si cresce o che si osserva. Questa e' quella in cui vengono elencati principi morali oggettivamente giusti e sbagliati, universali mi vien da dire. Si pone difatti sopra la *conventional morality* e la giudica/critica.
C'e' una questione importante che si pone: " **Voglio che la mia AI impari la morale di una particolare societa' in cui si e', oppure voglio che impari una morale che sia anche in grado di criticare la *conventional morality* di quella societa'? **". Credo di volere la seconda ovviamente (mia risposta personale).

Ci sono diversi tipi di *moralita'*:
1. **Normative ethics**: da' una risposta alle seguenti domande: Cosa rende le azioni giuste? Quali tratti del carattere contano come *virtu'* e quali come *difetti*, e perche'? Chi dovrebbe essere il nostro modello di comportamento? Il fine giustifica i mezzi oppure ci sono certe azioni che sono cosi' sbagliate che NON si dovrebbe MAI perpetrarle?
3. **Metaethics**:  Puo' una *teoria etica* essere **vera**? Cosa la rende vera? Posso ottenere saggezza morale? Se si', come? Ho sempre un buon motivo per compiere il mio *dovere morale*? Ci sono diversi personaggi che danno rispose a queste domande in maniera diversa. Ad esempio KANT dice che *possiamo capire cosa e' morale attraverso il ragionamento*, David Ross dice che possiamo attraverso l' *intuizione*.
Ci sono due visioni contrastanti poi:
1. *ASSOLUTISMO*:  quando due persone esprimono due giudizi etici **incompatibili** solo uno dei due giudizi puo' essere considerato **VERO**
2. *RELATIVISMO*: la verita' di quel GIUDIZIO e' pero' **circoscritta**/relativa a un particolare **framework** (non e' detto che quel *giudizio* rappresenti una **verita' assoluta**). Esempio: "l'aborto e' permissibile a livello morale" e' una frase che puo' essere vera sotto un certo tipo di **framework** e falsa secondo altri framework. (Immagino si riferisca al contesto di persone a cui questo giudizio venga esposto con *framework* non so).
**NOTA** la moralita' e' qualcosa su cui spesso ci sono questioni **discordanti**: se applicata a diverse *questioni*,puo' generare risposte etiche **molto** differenti tra loro. Ma ci sono comunque di **giudizi etici** considerabili *assoluti*?. Ad esempio:
1. E' sbagliato uccidere persone **innocenti**
2. E' sbagliato **fare del male a qualcuno**
Secondo me si (mia risposta personale).
Pero' ho molti *giudizi morali* che sono **defeasible**: ovvero che sono veri per la maggior parte dei casi ma ci sono alcune **eccezioni**. Ad esempio, guardando il caso di sopra "E' sbagliato fare del male a qualcuno". In generale si' ma se quel qualcuno magari sta per uccidere 3 membri della mia famiglia allora forse e' morale fargli del male.
**Voglio un AI che tenga in considerazione questa cosa del *defeasible*?** Credo di si' (mia  risposta personale)

Nelle slides c'e' un paragone tra la moralita' e altri sistemi normativi come *la legge*, *la religione*, *la tradizione* e l'*interesse individuale*. Per una spiegazione approfondita di quanto questi sistemi normativi differiscono dalla morale guardare _WHAT IS MORALITY_ in _READINGS_.