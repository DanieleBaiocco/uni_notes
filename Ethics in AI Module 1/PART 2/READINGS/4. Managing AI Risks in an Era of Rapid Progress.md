The pace of progress may surprise us again. Current deep learning systems still lack important capabilities and we do not know how long it will take to develop them. However, companies are engaged in a r**ace to create** generalist AI systems **that match or exceed human abilities in most cognitive work**.

There is no fundamental reason why AI progress would slow or halt when it reaches human-level abilities. Indeed, AI has already surpassed human abilities in narrow domains like protein folding and strategy games. Compared to humans, **AI systems can act faster, absorb more knowledge, and communicate at a far higher bandwidth**.

But we must anticipate the amplification of ongoing harms due to AI, as well as novel risks, and prepare for the largest risks well **before they materialize**. Climate change has taken decades to be acknowledged and confronted; for AI, decades could be too long.

# Societal-scale risks
AI systems could rapidly come to outperform humans in an increasing number of tasks. If such systems are not carefully designed and deployed, they pose various societal-scale risks. They threaten to amplify social injustice, erode social stability, and weaken our shared understanding of reality that is foundational to society. They could also enable large-scale criminal or terrorist activities. Especially in the hands of a few powerful actors, AI could cement or exacerbate global inequities, or facilitate **automated warfare**, customized **mass manipulation**, and **pervasive surveillance**.

Many of these risks could soon be amplified, and new risks created, as companies are developing **autonomous AI**: systems that can **plan**, **act** in the world, and **pursue goals**.

If we build highly advanced autonomous AI, we risk creating systems that pursue undesirable goals. Malicious actors could deliberately embed harmful objectives. 
Moreover, **no one currently knows how to reliably align AI behavior with complex values.**
Even well-meaning developers may **inadvertently build AI systems that pursue unintended goals**.
Once autonomous AI systems pursue undesirable goals, embedded by malicious actors or by accident, **we may be unable to keep them in check**: advanced autonomous AI systems will pose unprecedented control challenges.

To advance undesirable goals, future autonomous AI systems could use undesirable strategies—learned from humans or developed independently—as a means to an end.

In open conflict, AI systems could threaten with or use autonomous or biological weapons.

As autonomous AI systems *increasingly become faster and more costeffective than human workers*, a dilemma emerges. Companies, governments, and militaries might be **forced** to deploy AI systems widely and **cut back on expensive human verification of AI decisions**, or **risk being outcompeted**.
As a result, autonomous AI systems could increasingly assume critical societal roles. Without sufficient caution, we may irreversibly lose control of autonomous AI systems, rendering human intervention ineffective. Large-scale cybercrime, social manipulation, and other highlighted harms could then escalate rapidly. This unchecked AI advancement could culminate in a large-scale loss of life and the biosphere, and the marginalization or even extinction of humanity.

# A path forward
If advanced autonomous AI systems were developed today, we would not know how to make them safe, nor how to properly test their safety. Even if we did, governments would lack the institutions to prevent misuse and uphold safe practices. That does not, however, mean there is no viable path forward. To **ensure a positive outcome**, we can and must pursue **research breakthroughs** in AI safety and ethics and promptly **establish effective government oversight**.

## Reorienting technical Research and Development
Ci sono delle challenges, che c'e' da affrontare. Ci sono delle cose che vorrei che le AI avessero ma che non sempre hanno/avranno se puntiamo solo sul renderle piu' **capaci e intelligenti**:
1. *oversight and honesty*: una AI piu' capace can better exploit weaknesses in oversight and testing, for example, by producing false but compelling output
2. *Robustness*: AI systems behave unpredictably in new situations (under distribution shift or adversarial inputs)
3. *Interpretability and transparency*: AI decisionmaking is opaque. So far, we can only test large models via trial and error. We need to learn to understand their inner workings
4. *Inclusive AI development*: AI advancement will need methods to mitigate biases and integrate the values of the many populations it will affect
5. *Risk evaluations*: Frontier AI systems develop unforeseen capabilities only discovered during training or even well after deployment. Better evaluation is needed to detect hazardous capabilities earlier.
6. *Addressing emerging challenges*: More capable future AI systems may exhibit failure modes we have so far seen only in theoretical models. AI systems might, for example, learn to feign obedience or exploit weaknesses in our safety objectives and shutdown mechanisms to advance a particular goal

Given the stakes, we call on major tech companies and public funders to allocate at least one-third of their AI R&D budget to ensuring safety and ethical use, comparable to their funding for AI capabilities.

## Governance measures
We urgently need national institutions and international governance to enforce standards to prevent recklessness and misuse. Many areas of technology, from pharmaceuticals to financial systems and nuclear energy, show that society requires and effectively uses governance to reduce risks. However, no comparable governance frameworks are currently in place for AI. Without them, companies, militaries, and governments may seek a competitive edge by pushing AI capabilities to new heights while cutting corners on safety, or by delegating key societal roles to AI systems with little human oversight. Like manufacturers releasing waste into rivers to cut costs, they may be tempted to reap the rewards of AI development while leaving society to deal with the consequences.

To protect low-risk use and academic research, they should avoid undue bureaucratic hurdles for small and predictable AI models. The most pressing scrutiny should be on AI systems at the frontier: a small number of most powerful AI systems— trained on billion-dollar supercomputers—which will have the most hazardous and unpredictable capabilities

For AI systems with hazardous capabilities, we need a combination of governance mechanismsmatched to the magnitude of their risks. Regulators should create national and international safety standards that depend on model capabilities.

Further measures are needed for exceptionally capable future AI systems, such as autonomous systems that could circumvent human control. Governments must be prepared to license their development, restrict their autonomy in key societal roles, halt their development and deployment in response to worrying capabilities, mandate access controls, and require information security measures robust to state-level hackers, until adequate protections are ready.

To bridge the time until regulations are complete, major AI companies should promptly lay out if-then commitments: specific safety measures they will take if specific **red-line capabilities** are found in their AI systems.

While AI capabilities are advancing rapidly, progress in safety and governance is lagging behind. To steer AI toward positive outcomes and away from catastrophe, we need to reorient. There is a responsible path, if we have the wisdom to take it.