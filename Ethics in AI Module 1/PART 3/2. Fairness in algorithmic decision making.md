In tanti *domains* si ha che **predizioni automatiche** date da una AI non solo sono piu' **economiche** rispetto a avere un gruppo di persone che fa quel tipo di scelte, ma spesso sono anche **piu' precise e imparziali** rispetto agli umani:
1. L'AI puo' evitare i tipici errori della **psicologia umana** come essere over-confidenti, avere dei bias e evitare l'**inabilita' da parte degli umani di processare dati statistici**, oltre all'evitare **i pregiudizi umani** (riguardo ad esempio al sesso, all'etnia).
2. In molti campi si e' proprio visto che sistemi algoritmici (come AI systems) hanno spesso performato meglio rispetto a esperti umani
Ci sono stati casi pero' di AI che erano *discriminatorie*. In pochi casi c'e' stata una discriminazione esplicita, chiamata *disparate treatment*, in cui l'AI basava i suoi outcomes su features proibite come la razza, l'etnia, il gender.
Si tende piu' a associare un outcome a una razza se il dataset e' fatto per cui la razza e' legata a un certo tipo di outcome **causa discriminazione razziale, causa contesto sociale INGIUSTO**.


Dei sistemi basati su *supervised learning* possono infatti esser trainati su *giudizi umani passati* e quindi possono *riprodurre* i punti di forza ma soprattutto quelli di **debolezza** degli umani che hanno dato quel tipo di giudizio, che includono ovviamente la propensione di alcuni di esser pregiudizievoli verso determinate etnie,ecc...
* For example, a recruitment system trained on the past hiring decisions will learn to emulate the managers’ assessment of the suitability of candidates, rather than to directly predict an applicant’s performance at work. If past decisions were influenced by prejudice, the system will reproduce the same logic.

# Le sorgenti di UNFAIRNESS nei sistemi di AI
- certi sistemi sonovolontariamente costruiti PER ESSERE BIASED (molto raro)
- certi sistemi vengono allenati su datasets passati legati a giudizi umani caratterizzati da pregiudizio e ingiustizia (come datasets legati all'assunzione di personale (l'assunzione di un uomo e' piu' vantaggiosa di quella di una donna), a decisioni di prestito (a chi prestare i soldi: ci potrebbero essere biases su razza e genere)).
- certi sistemi risolvono un problema non discriminatorio ma usando per il training un proxy (un dataset) che invece e' discriminatorio. Un esempio potrebbe essere un sistema AI che predice le **performance a lavoro**, che e' una task non discriminatoria, pero' magari usando un dataset che rifletta dei biases. Magari infatti c'e' una feature che e' legata a quanto quella persona lavora (quante ore) e ho come target quello di predictare la performance lavorativa di una persona. Nel passato le donne lavoravano molte meno ore rispetto agli uomini. Di conseguenza se utilizzo un dataset in cui ci sono donne che lavorano meno ore, avro' come risultato che la mia AI dira' che una donna e' molto meno performante a lavoro rispetto a un uomo.
- quando ci sono nel dataset preso in considerazione delle features favorevoli o non favorevoli che si applicano **SOLO** a **certi gruppi**.
- quando ci sono dei datasets che non riflettono dei comportamenti che avvengono nella societa' in cui si vive  (nella vita di tutti i giorni),  come il fatto che certi gruppi (i negri) vengono magari fermati di piu' dalla polizia per controlli piu' stringenti. Un dataset in cui questa informazione di accanimento verso i negri non e' encodata  sara' biased.
- quando ho dei datasets che non riflettono evoluzioni positive della nostra societa'
- quando ho dei datasets in cui certi gruppi sono **sottorappresentati** rispetto agli altri. Questo riduce l'accuracy di quel gruppo rispetto agli altri. 
- quando ho predizioni che arrecano un danno a persone gia' svantaggiate (come ad esempio una persona che gia' e' messa male economicamente, metti che c'e' anche un'altra AI che gli predice che tra poco morira', allora il prestito NON glielo faccio, ma cio' e' discriminatorio e immorale)

## Pregiudizio nel training set
Il pregiudizio e la discriminazione all'interno del dataset puo' anche essere presente nonostante le *prohibit features* vengano escluse (quindi features riguardanti il sesso, la razza, la regione di appartenenza, ecc...).
Infatti basta che ci siano **correlazioni**, anche NON dirette, tra features discriminatorie e predizioni del training set per avere discriminazioni. 
Ad esempio si potrebbe avere un *dataset* in cui NON e' presente la feature della razza a tempo di training MA che e' riferito a assunzioni fatte da un manager con grossi pregiudizi. Questo manager NON ha mai assunto persone negre. Queste persone abitavano/continuano a abitare nello stesso quartiere. La feature del quartiere e' pero' presente a tempo di training. Di conseguenza quando si faranno predictions, dato l'input, si sara' discriminatori verso il quartiere di provenienza (e quindi anche molto probabilmente verso i negri che in maggior parte abitano in quel quartiere).

## Sistemi discriminatori verso un gruppo
In other cases, a training set may be biased against a certain group, since the achievement of the outcome being predicted (e.g., job performance) is approximated through a proxy that has a disparate impact on that group.
	• Assume, for instance, that the future performance of employees (the target of interest in job hiring) is only measured by the number of hours worked in the office. This outcome criterion will lead to past hiring of women —who usually work for fewer hours than men, having to cope with family burdens— being considered less successful than the hiring of men; based on this correlation (as measured on the basis of the biased proxy), the systems will predict a poorer performance of female applicants.
## Challenging the unfairness of automated decision-making
These criticisms have been countered by observing that algorithmic systems, even when based on machine learning, are more controllable than human decision-makers, their faults can be identified with precision, and they can be improved and engineered to prevent unfair outcomes.

La cosa assurda da capire e' questa: a biased algorithmic system can still be fairer than an even more biased human decisionmaker

In many cases, the best solution consists in integrating human and automated judgements, by enabling the affected individuals to request a human review of an automated decision as well as by favouring transparency and developing methods and technologies that enable human experts to analyse and review automated decision-making.

The future challenge will consist in finding the best combination between human and AI, taking into account the capacities and the limitations of both.


