**Noise**: Modification of original values. Uninteresting mixed to the interesting data.
**OUTLIERS** Data whose characteristics are considerably different from most of the data in the dataset. Can be generated by noise or rare events/processes.

With box plots, I have a way to find outlier values using quartiles.
COSE DA USARE: pairplots, boxplots, histograms (che sono inclusi nei pair plots praticamente)
# Exploration of the dataset
## Boxplots

Boxplots, also known as box-and-whisker plots, are a graphical way of summarizing the distribution of a dataset. They provide a visual representation of the central tendency, spread, and skewness of the data, as well as any potential outliers. Here's how they work:

1. **Central Tendency**: The box in a boxplot represents the interquartile range (IQR), which is the middle 50% of the data. The line inside the box represents the median (or the 50th percentile) of the dataset.

2. **Spread**: The length of the box indicates the spread of the data within the interquartile range. The longer the box, the more spread out the data is.

3. **Whiskers**: The "whiskers" extend from the edges of the box to indicate the range of the data outside the interquartile range. By default, the whiskers typically extend 1.5 times the IQR from the first (lower) and third (upper) quartiles. Any data points beyond the whiskers are considered potential outliers.

4. **Outliers**: Outliers are individual data points that are significantly different from the rest of the dataset. In a boxplot, they are usually plotted as individual points beyond the whiskers.

5. **Symmetry and Skewness**: Boxplots can also reveal the symmetry or skewness of the data distribution. If the median line is close to the center of the box, the data is symmetrically distributed. If the median line is closer to one end of the box, the data may be skewed in that direction.


![[singleBoxplot.png]]
When you have a dataset with multiple features and corresponding classes or groups, you can create separate boxplots for each feature, divided by the classes or groups. This approach helps to understand how the distribution of values differs among the classes. In addition to comparing distributions across different datasets or groups, boxplots are valuable for understanding how the distribution of a particular feature varies within different classes or **categories**. This may reveal any discrepancies or patterns that may exist.
By examining these boxplots, you can identify which features exhibit significant differences in distribution among the classes. This information is valuable for understanding how each feature contributes to distinguishing between the classes and can guide feature selection, model building, and further analysis in machine learning and data analysis tasks.

![[boxplots_deeper.png]]
## Pairplots 

Pairplots, particularly when incorporating different classes or categories, provide a comprehensive view of the relationships between variables in a dataset while considering the class distinctions. Here's how pairplots with class distinctions work and how they aid in dataset exploration:

1. **Visualization of Pairwise Relationships with Classes**: In pairplots with class distinctions, each scatter plot represents the relationship between two variables, as usual. However, in addition to plotting the data points, each point is colored or marked based on the corresponding class or category to which it belongs. This allows for the visualization of how different classes are distributed within each pairwise relationship.

2. **Identification of Class-Specific Patterns**: By examining the scatter plots with class markings, you can identify class-specific patterns, trends, and associations between variables. These visualizations help in understanding how different classes behave with respect to each other and how they may influence the relationships between variables.

3. **Correlation and Association within Classes**: Pairplots with class distinctions enable the exploration of correlations and associations within each class separately. You can observe whether the relationships between variables differ across classes and whether certain variables have stronger associations within specific classes.

4. **Insights into Class Separability**: Pairplots with class distinctions can provide insights into the **separability of classes** based on the variables under consideration. You can visually assess whether the classes are well-separated or if there is overlap in the distributions, which can inform classification and prediction tasks.

5. **Detection of Outliers and Anomalies within Classes**: Just like in traditional pairplots, pairplots with class distinctions allow for the identification of outliers and anomalies within each class. This is particularly useful for understanding the characteristics of outliers within specific classes and their potential impact on modeling tasks.

Overall, pairplots with class distinctions offer a holistic view of the relationships between variables in a dataset while considering the class information. They help in identifying class-specific patterns, correlations, and separability, making them valuable tools for exploratory data analysis and classification tasks.
# Classification
## Decision Trees 
Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They are powerful yet interpretable models that mimic human decision-making. 
### Structure
A decision tree consists of nodes, branches, and leaves: 
- **Nodes**: Represent features or attributes that are used to make decisions.
- **Branches**: Connect nodes and represent the possible outcomes of decisions based on feature values. 
- **Leaves**: Terminal nodes that represent the final decision or prediction. 
### Entropy and Information Gain in Decision Trees

In decision trees, entropy and Information Gain (IG) are key concepts used to determine the best split criteria for partitioning the data. Let's break down each concept:

1. **Entropy**:
   - Entropy is a measure of impurity or uncertainty in a dataset. In the context of decision trees, entropy is used to quantify the randomness of the target variable's distribution within a subset of data.
   - Mathematically, for a binary classification problem with classes {0, 1}, the entropy of a node S is calculated as:
     ![H(S) = - p_1 log2(p_1) - p_0 log2(p_0)](https://latex.codecogs.com/svg.latex?H(S)%20%3D%20-%20p_1%20%5Clog_2%28p_1%29%20-%20p_0%20%5Clog_2%28p_0%29)
     where \( p_1 \) is the proportion of samples belonging to class 1 and \( p_0 \) is the proportion of samples belonging to class 0.
   - In a multi-class classification problem with \( K \) classes, the entropy formula is extended accordingly.

2. **Information Gain (IG)**:
   - Information Gain is a metric used to quantify the effectiveness of a feature in reducing uncertainty (i.e., entropy) within a dataset.
   - The IG of a feature is calculated as the difference between the entropy of the parent node and the weighted average of entropies of its child nodes after splitting on that feature.
   - Mathematically, for a feature A, the Information Gain \( IG(A) \) is computed as:
     ![IG(A) = H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\cdot H(S_v)](https://latex.codecogs.com/svg.latex?IG%28A%29%20%3D%20H%28S%29%20-%20%5Csum_%7Bv%20%5Cin%20Values%28A%29%7D%20%5Cfrac%7B%7CS_v%7C%7D%7B%7CS%7C%7D%20%5Ccdot%20H%28S_v%29)
     where:
     - \( H(S) \) is the entropy of the parent node before the split.
     - \( Values(A) \) represents the possible values of feature \( A \).
     - \( |S| \) is the total number of samples in the parent node.
     - \( |S_v| \) is the number of samples in the child node corresponding to value \( v \) of feature \( A \).
     - \( H(S_v) \) is the entropy of the child node corresponding to value \( v \) of feature \( A \).
   
3. **Decision Tree Splitting**:
   - In a decision tree, the feature with the highest Information Gain is selected as the splitting criterion at each node.
   - The tree recursively splits the data based on the selected feature until certain stopping criteria are met, such as reaching a maximum depth, minimum number of samples per leaf, or no further improvement in Information Gain.

In summary, entropy quantifies the randomness or impurity within a dataset, and Information Gain measures the effectiveness of a feature in reducing that uncertainty when used for splitting in a decision tree. By selecting features with the highest Information Gain, decision trees can efficiently partition the data into subsets that are increasingly homogeneous with respect to the target variable.
### Gini Index in Decision Trees

The Gini index is another impurity measure used in decision trees, similar to entropy. It quantifies the impurity of a dataset by measuring the probability of incorrectly classifying a randomly chosen element in the dataset. Here's a breakdown of the Gini index in decision trees:

1. **Definition**: 
   - The Gini index measures the impurity of a dataset \( S \) with respect to its class distribution. For a binary classification problem with classes \( \{0, 1\} \), the Gini index of a node \( S \) is calculated as:
     $$G(S) = 1 - \sum_{i=1}^{C} p_i^2 $$
     where \( p_i \) is the proportion of samples in class \( i \) within node \( S \), and \( C \) is the total number of classes.
   - In a multi-class classification problem with \( K \) classes, the Gini index formula is extended accordingly.

2. **Interpretation**:
   - A Gini index of 0 indicates perfect purity, where all elements belong to the same class.
   - A Gini index closer to 1 indicates higher impurity, where elements are evenly distributed among classes.

3. **Splitting Criteria**:
   - In decision trees, the Gini index is used to evaluate the purity of a split. The feature and threshold that result in the lowest Gini index (i.e., the highest purity) are chosen as the splitting criteria.
   - The Gini index is calculated for each possible split, and the feature and threshold combination that minimizes the weighted sum of the Gini indices for the resulting child nodes is selected.

4. **Comparison with Entropy**:
   - Both Gini index and entropy are commonly used impurity measures in decision trees.
   - While entropy tends to be slightly more sensitive to changes in class probabilities, the Gini index is computationally faster to calculate.
   - In practice, the choice between Gini index and entropy often depends on specific use cases and computational considerations.

In summary, the Gini index is a measure of impurity used in decision trees to evaluate the quality of splits. It quantifies the probability of misclassifying a randomly chosen element in a dataset and is used to select the best splitting criteria during tree construction.

### Working Principle 
1. **Splitting**: The decision tree algorithm selects the best feature to split the data at each node. This is done based on criteria such as IG (as seen above), Gini impurity (as seen above) ecc...
2. **Recursive Partitioning**: The dataset is recursively partitioned into subsets based on the selected feature until certain stopping criteria are met, such as reaching a maximum tree depth, minimum number of samples per leaf, or no further improvement in impurity reduction. 3. **Prediction**: To make predictions, new data points traverse the tree from the root to a leaf node based on the feature values. The predicted class or value at the leaf node is then assigned to the data point. 
3. this is the pseudocode:
	Procedure `buildTree(dataset X, node p)`:
	    If all class values of X are the same:
	        Return node p as a leaf, with label of p set to the common class value
	    End If
	
	    If no attribute can give a positive information gain in X:
	        Set c to the majority class value in X
	        Return node p as a leaf, with label of p set to c
	    End If
	
	    Find the attribute d and threshold t that give maximum information gain,
	    or lowest Gini Index in X.
	    
	    Create two internal nodes descendant of p, named p_left and p_right
	
	    Let X_left be the subset of X where attribute d is less than t
	    Let X_right be the subset of X where attribute d is greater than or equal to t
	
	    Recursively call `buildTree(X_left, p_left)`
	    Recursively call `buildTree(X_right, p_right)`
	
	    Return node p with its left child set to p_left and right child set to p_right
	End Procedure

### Advantages 
- **Interpretability**: Decision trees provide intuitive explanations of decisions. 
- **Handles Non-linear Relationships**: They can capture complex non-linear relationships in the data.
- **Feature Importance**: Decision trees can identify important features for prediction. 
- **Robust to Outliers**: They are robust to outliers and missing values.
### Limitations 
- **Overfitting**: Decision trees are prone to overfitting, especially with deep trees. 
- **Instability**: Small variations in the data can result in significantly different trees.
- **Bias Towards Features with Many Levels**: Features with many levels tend to be favored in the splitting process.
### Statistical Pruning of Decision Trees with Error Estimation

Statistical pruning of decision trees with error estimation is a technique used to optimize decision tree models by reducing their complexity while maintaining good predictive performance. Here's how it typically works:

1. **Building the Tree**: Initially, a decision tree is built using an algorithm such as ID3, C4.5, or CART. This tree is typically grown to its maximum depth or until certain stopping criteria are met, resulting in a potentially complex and overfitted tree.

2. **Pruning**: After the tree is built, pruning is performed to simplify it. Instead of relying solely on heuristic measures like minimum sample size per leaf or maximum depth, statistical pruning techniques use statistical tests to determine whether pruning a subtree would lead to a significant decrease in predictive performance.

3. **Error Estimation**: Before pruning, the performance of each subtree (including the entire tree) is estimated using a holdout set or through cross-validation. This provides an estimate of the error rate associated with each subtree.

4. **Statistical Tests**: Statistical tests, such as chi-squared test or G-statistic, are then applied to evaluate whether pruning a subtree would result in a statistically significant increase in error rate compared to keeping it. If pruning a subtree does not lead to a significant increase in error rate, it is pruned. As we can see from this example I have that: ![[treePruning.png]]
	and that if the error of pruning is smaller than Pruning is a good choice:
	![[treePruning2.png]]

6. **Iterative Process**: This process is typically iterative, starting from the leaves and moving towards the root of the tree. Subtrees are pruned one by one, and the statistical tests are re-evaluated at each step to determine whether further pruning is beneficial.


By using statistical tests to guide the pruning process, this approach ensures that the resulting pruned tree maintains good predictive performance while being more interpretable and less prone to overfitting compared to the fully grown tree.

Overall, statistical pruning with error estimation is a powerful technique for optimizing decision tree models, striking a balance between complexity and predictive accuracy.

## Cross Validation with k folds 
Cross-validation with k folds is a technique used in machine learning to evaluate the performance of a model on a dataset. Here's how it works:
1. **Splitting the data**: First, the original dataset is divided into k equal-sized subsets, or "folds".
2. **Training and validation**: The model is trained k times. In each iteration, k-1 folds are used for training, and the remaining fold is used for validation. So, for each iteration, a different fold is used as the validation set while the rest are used for training. 
3. **Evaluation**: After training the model on k-1 folds and validating it on the remaining fold, the performance metric (such as accuracy, precision, recall, etc.) is computed. This process is repeated k times, each time with a different fold held out for validation. 
4. **Average performance**: Finally, the performance metrics obtained from each iteration are averaged to obtain a single estimation of the model's performance. Cross-validation helps to provide a more accurate estimate of the model's performance compared to a single train-test split because it utilizes the entire dataset for both training and validation, ensuring that each data point is used for validation exactly once. This helps to reduce bias and provides a more reliable assessment of the model's generalization capability. Common choices for k include k = 5 and k = 10, but the choice depends on the size of the dataset and computational resources available. Smaller k values might result in higher variance in the performance estimate, while larger k values might be computationally expensive.
Is it recommended  (it is in the slides) to do cross validation with k folds along with grid search. The best set of hyperparameters is then used to retrain the model this time without the k-folds but using the entire train set. Then the learnt model will be used to make predictions on the test set.

### Train- Validation -Test split
1. The train/validation loop is faster than the Cross Validation
2. The optimisation of the hyperparameters is done with the validation set, independent from the final evaluation
3. It is the one I use in deep learning every time.
## Metrics
### Accuracy 
$$\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}} \times 100\% $$

- **Imbalanced Datasets**: Accuracy may not provide an accurate assessment of model performance in datasets where the classes are unevenly distributed. In such cases, a high accuracy score may be misleading if the model **predominantly predicts the majority class**.
  
- **Additional Metrics**: It's important to complement accuracy with other evaluation metrics such as **precision, recall, F1-score, and the confusion matrix**. These metrics provide insights into different aspects of the model's performance, such as its ability to correctly identify positive instances (precision) and its ability to capture all positive instances (recall).
### Precision:
Precision measures the proportion of true positive predictions among all positive predictions made by the model.

$$ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} $$

### Recall:
Recall measures the proportion of true positive predictions among all actual positive instances in the dataset.

$$ \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}} $$

### F1-score:
F1-score is the harmonic mean of precision and recall. It provides a balanced measure of a model's performance by considering both precision and recall.

$$ \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$

These formulas use true positives (TP), false positives (FP), and false negatives (FN) to calculate precision, recall, and F1-score. Each metric provides valuable insights into different aspects of a classification model's performance.
F1–score is always interesting, because has higher values when precision and recall are reasonably balanced.
If the the **costs of errors on positives and negatives are significantly different**, then it is necessary to evaluate singularly one of precision and recall (the one in which we are interested based on our deploy target and problem specifics).


In classification tasks, precision, recall, and F1-score are inherently defined for individual classes. In binary classification, they are typically associated with the positive class. However, in multi-class scenarios, classifiers in libraries like `scikit-learn` generate an array of scores, with each value corresponding to a specific class.

During model evaluation and hyperparameter tuning processes such as `GridSearchCV`, it's often necessary to obtain a **single value to optimize the hyperparameters**. In such cases, if we aim to maximize one of precision, recall, or F1-score, an **average value** across **all classes** becomes necessary.

### How to  average in multiclass confusion matrixes, in order to obtain a single value

with **wighted average** the measure of each class influences the result in proportion with its size, therefore in case of imbalancing the minority classes have smaller influence

#### Macro Average:

- **Description**: Macro average computes the metric independently for each class and then takes the unweighted average of those values.
- **Use Case**: It treats all classes equally and is useful when each class is of equal importance.
- with **macro average** the measure of each class has the same impact on the average value, therefore in case of imbalancing the minority classes have influence bigger than their size
- $$ Macro Average = (Metric(Class 1) + Metric(Class 2) + ... + Metric(Class N)) / N
 $$ 
#### Micro Average:

- **Description**: Micro average aggregates the contributions of all classes to compute the metric.
- $$ Micro Average = Total_{TP} / (Total_{TP} + Total_{FP}), Total_{FN} $$
#### Weighted Average:

- **Description**: Weighted average calculates the metric for each class, weighted by the number of instances in each class, and then averages them.
-  **Use Case**: It considers the imbalance in class distributions and gives more weight to classes with more instances.
- **Formula**:
- $$ Weighted Average = (Metric(Class 1) * Weight(Class 1) + Metric(Class 2) * Weight(Class 2) + ... + Metric(Class N) * Weight(Class N)) / Total Instances
 $$
These averages are commonly used in classification tasks to summarize model performance across multiple classes. Each average has its own strengths and use cases, and the choice depends on the specific requirements of the problem at hand.


### Cohen's Kappa (κ) in Machine Learning Using Confusion Matrix

In machine learning, Cohen's Kappa (κ) can be applied to assess the level of agreement between predicted and true labels using a confusion matrix. The confusion matrix is a tabular representation of the performance of a classification model, detailing the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions.

#### How Cohen's Kappa is Applied to a Confusion Matrix:

1. **Calculate Observed Agreement (\(P_o\)):**
   - \(P_o\) is the proportion of observed agreement between the predicted and true labels.
   - $$P_o = \frac{TP + TN}{TP + TN + FP + FN}$$

2. **Calculate Expected Agreement (\(P_e\)):**
   - \(P_e\) is the proportion of agreement expected by chance alone. It considers the marginal probabilities of agreement for each class.
   - For a binary classification (two classes A and B):
     $$P_e = \frac{(TP + FN) \times (TP + FP)}{(TP + TN + FP + FN)^2} + \frac{(FP + TN) \times (FN + TN)}{(TP + TN + FP + FN)^2}$$

3. **Compute Cohen's Kappa (\(κ\)):**
   - Use the formula: $$\kappa = \frac{P_o - P_e}{1 - P_e}$$

#### Interpretation:
- \(κ\) values close to 1 indicate a high level of agreement beyond chance.
- \(κ\) values around 0 suggest agreement equal to that expected by chance alone.
- Negative \(κ\) values indicate disagreement, potentially worse than chance.

#### Use Cases in Machine Learning:
- **Model Evaluation:**
  - Cohen's Kappa provides a robust measure of classification agreement beyond accuracy, especially when dealing with imbalanced datasets.
  - It helps assess the model's performance while considering the possibility of random agreement.

- **Hyperparameter Tuning:**
  - In scenarios like hyperparameter tuning using techniques like GridSearchCV, where a single metric is needed for optimization, \(κ\) can be used to provide a comprehensive assessment.

- **Comparison of Models:**
  - When comparing multiple models or algorithms, \(κ\) can offer insights into their agreement with true labels, accounting for chance agreement.

Applying Cohen's Kappa in machine learning provides a more nuanced understanding of classification performance, particularly in situations where accuracy alone might be insufficient, such as imbalanced datasets or scenarios with unequal class distributions.

## Evaluation of PROBABILISTIC CLASSIFIERS
Many classifiers produce, rather than a class label (crisp prediction), a tuple of probabilities, one for each possible class, (probabilistic, or soft prediction).
**Crisp values sometimes hide probabilities**:   when a leaf of a decision tree has non–zero counts for the minority classes a less–than–one probability, the probabilities of the examples falling in that leaf can be assigned on the basis of the fractions of the training data elements in that leaf belonging to each class

**Probabilities** can be converted to a **crisp value** with different techniques, depending on the number of classes (binary or multiclass) binary 
1. set a threshold for the positive class in the case of *binary classification*
2. output the class with the maximum probability, in the case of *multiclass classification*
### Lift Chart
A lift chart, also known as a gain chart or response chart, is a visual tool used in marketing and data analysis to evaluate the performance of predictive models or targeting strategies. It's particularly common in customer relationship management (CRM) and direct marketing.

Here's how it works:

1. **Ranking**: The data set is ranked based on the predicted probabilities or scores generated by the predictive model. For example, if you're predicting the likelihood of a customer responding to a marketing campaign, you rank customers from most likely to least likely to respond.
    
2. **Dividing into Segments**: The ranked data set is divided into equal segments, usually deciles (10 segments) or quintiles (5 segments). Each segment contains an equal number of observations.
    
3. **Calculating Cumulative Response**: For each segment, you calculate the cumulative response rate. This is the proportion of actual responses (e.g., purchases, conversions) within that segment.
    
4. **Plotting the Chart**: On the lift chart, you plot the cumulative response rate for each segment against the segment number. The lift chart typically starts at 0% response rate for the first segment and ends at 100% for the last segment.
    
5. **Interpreting the Chart**: A good predictive model or targeting strategy will result in a lift curve that is steeper than the baseline (random) curve. The area between the model's lift curve and the baseline curve represents the improvement in response rate achieved by using the model or strategy. The higher the lift at any given segment, the more effective the model or strategy is at targeting the desired outcomes compared to random targeting.
    

In summary, a lift chart helps you understand how much better your model or targeting strategy is performing compared to random chance, across different segments of your audience.

### ROC
An ROC (Receiver Operating Characteristic) curve is another common tool used in evaluating the performance of predictive models, particularly in binary classification tasks. Here's an explanation of how it works:

1. **True Positive Rate (Sensitivity)**: On the y-axis, you have the true positive rate (TPR), also known as sensitivity. This is the proportion of actual positive cases that are correctly identified by the model. It's calculated as the number of true positives divided by the sum of true positives and false negatives.
    
2. **False Positive Rate (1 - Specificity)**: On the x-axis, you have the false positive rate (FPR), which is calculated as 1 - specificity. Specificity represents the proportion of actual negative cases that are correctly identified as negative by the model. False positive rate is the proportion of negative cases incorrectly classified as positive.
    
3. **Plotting Points**: The ROC curve is created by plotting the TPR (sensitivity) against the FPR (1 - specificity) at various threshold settings for the classification model. Each point on the curve represents a different threshold for classifying positive cases.
    
4. **Diagonal Line and Random Classifier**: The diagonal line from (0,0) to (1,1) represents the ROC curve of a random classifier, where the true positive rate is equal to the false positive rate. A model that performs no better than random chance would fall along this line.
    
5. **Ideal Performance**: A perfect classifier would have a curve that goes straight up to the top-left corner (TPR = 1, FPR = 0), indicating that it achieves perfect sensitivity (all positives correctly identified) with no false positives.
    
6. **Area Under the Curve (AUC)**: The area under the ROC curve (AUC) is a common metric used to quantify the overall performance of the model. A perfect classifier would have an AUC of 1, while a random classifier would have an AUC of 0.5. The higher the AUC, the better the model's ability to distinguish between positive and negative cases.
    

In summary, the ROC curve provides a visual representation of the trade-off between sensitivity and specificity for different threshold settings of a binary classification model, and the AUC summarizes the overall performance of the model across all possible thresholds
## Naive Bayes Classifier
1. **Training Phase**: During training, Naive Bayes calculates the prior probabilities of each class (the probability of each class occurring in the dataset) and the likelihood of each feature given each class. These probabilities are estimated from the training data.
    
2. **Prediction Phase**: When a new instance is presented for classification, Naive Bayes calculates the posterior probability of each class given the observed features using Bayes' theorem. The class with the highest posterior probability is then predicted as the label for the instance.
### Building Prior Probabilities and Likelihoods

**1. Prior Probabilities:**
- Prior probabilities represent the probability of each class occurring in the dataset without considering any features.
- To compute the prior probabilities, you count the number of instances belonging to each class in the training data and divide by the total number of instances.
- Mathematically, for a class $c$, the prior probability $P(c)$ is calculated as:
  $$ P(c) = \frac{{\text{{Number of instances of class }} c}}{{\text{{Total number of instances}}}} $$

**2. Likelihoods:**
- Likelihoods represent the probability of observing each feature given the class.
- In Naive Bayes, the likelihoods are typically calculated under the assumption of feature independence given the class.
- For each feature  and each class, you count the number of times that feature occurs in instances of that class and divide by the total count of all features in instances of that class.
- Mathematically, for a feature $x$ and a class $c$, the likelihood $P(x|c)$ is calculated as:
  $$ P(x|c) = \frac{{\text{{Number of occurrences of feature }} x \text{{ in instances of class }} c}}{{\text{{Total count of all features in instances of class }} c}} $$

**3. Smoothing:**
- In practice, it's common to encounter features that do not appear in instances of a particular class in the training data. This can result in zero probabilities, which can cause issues during classification.
- Smoothing techniques, such as Laplace smoothing (additive smoothing), are often applied to the likelihood estimates to prevent zero probabilities.
- Laplace smoothing involves adding a small constant value to the numerator and adjusting the denominator to account for the added counts. This ensures that no feature has zero probability for any class.

**4. Handling Continuous Features:**
- If the features are continuous (e.g., word frequencies), you may need to assume a probability distribution for each feature given the class, such as Gaussian distribution for Gaussian Naive Bayes.
- In this case, you estimate the parameters of the distribution (e.g., mean and variance for Gaussian distribution) based on the instances of each class in the training data.

**5. Combining Prior Probabilities and Likelihoods:**
- During classification, the prior probabilities and likelihoods are combined using Bayes' theorem to compute the posterior probability of each class given the observed features.
- The class with the highest posterior probability is then predicted as the label for the instance.

When the fundamental assumptions of Naive Bayes are not met, it can lead to a **DRAMATIC DEGRADATION** in performance:

1. **Violation of Independence**:
    - If attributes are not truly independent given the class label, such as when one attribute is merely a copy of another or a linear transformation of another, Naive Bayes can suffer.
    - The algorithm relies heavily on the assumption of feature independence, and violations of this assumption can result in inaccurate probability estimates and poor classification performance.
2. **Violation of Gaussian Distribution**:
    - Naive Bayes assumes that continuous features follow a Gaussian (normal) distribution, or some other known probability distributions. But It could be the case that the programmer **doesn't undestand**  the distribution of a feature.
    - In such cases, using Kernel Density Estimation (KDE) or other estimation procedures becomes necessary. KDE allows for non-parametric estimation of the probability density function of the feature, which can capture more complex distributions.

In both cases, failing to address these issues can significantly degrade the performance of Naive Bayes. 

## Linear classification with Perceptron
Here we want to **learn a hyperplane** such that **all the positives** lay on one side and **all the negatives** on the other.
The hyperplane is described by a set of weights $$ w_0, \ldots, w_D $$ in a linear equation on the data attributes $$ x_0, \ldots, x_D$$.
the fictitious attribute x0=1 is added to allow a hyperplane that does not pass through the origin. The weight w0 that is multiplied to x0 is the bias weight.

The hyperplane is described by the linear equation:

$$w_0 \cdot x_0 + w_1 \cdot x_1 + \ldots + w_D \cdot x_D = 
\begin{cases} 
\text{POSITIVE,} & \text{if } > 0 \\
\text{NEGATIVE,} & \text{if } < 0 
\end{cases} $$

Learning the hyperplane with a linear perceptron involves an iterative process of adjusting the weights to correctly classify the data into two classes. Here's a step-by-step explanation:
WROTE A LITTLE BIT BETTER: 
1. **Initialization**:
   - Initialize the weights \( $$ w_0, \ldots, w_D $$ \) to small random values or zeros.
   - Set the bias term $$ w_{0} $$ to a constant value or zero.

2. **Training Iterations**:
   - Iterate through the training data samples until convergence or a predefined number of iterations:
   
     a. **For each data sample**:
        - Input the feature vector $$x = [x_0, x_1, \ldots, x_D] $$ (including the bias term $$ x_0 = 1 $$).
        - Compute the activation of the perceptron: $$z = w_0 \cdot x_0 + w_1 \cdot x_1 + \ldots + w_D \cdot x_D $$.
        - Compute the predicted class label:
          $$
          \hat{y} =
          \begin{cases}
          1, & \text{if } z > 0 \\
          -1, & \text{if } z \leq 0
          \end{cases}
          $$
          
     b. **Update the weights if misclassified**:
        - If the predicted class label (\( \hat{y} \)) does not match the actual class label (\( y \)), update the weights:
          $$
          w_i = w_i + \eta \cdot (y - \hat{y}) \cdot x_i
          $$
          where $$ \eta $$ is the learning rate and y is the true class label.
          
3. **Convergence**:
   - Repeat the training iterations until convergence, which occurs when all data samples are correctly classified or a maximum number of iterations is reached.
   - Convergence may not always be achieved if the data is not linearly separable or if the learning rate is too large.

4. **Decision Boundary**:
   - The learned hyperplane separates the data into two classes. The decision boundary is defined by the equation:
     $$
     w_0 \cdot x_0 + w_1 \cdot x_1 + \ldots + w_D \cdot x_D = 0
     $$
   - The weights $$w_1, w_2, \ldots, w_D $$ determine the orientation and position of the decision boundary in the feature space.

5. **Evaluation**:
   - After training, evaluate the performance of the perceptron on a separate validation or test dataset to assess its classification accuracy.

The linear perceptron is a simple yet effective algorithm for binary classification tasks, especially when the data is linearly separable. However, it has limitations, such as being sensitive to outliers and **not being able to handle non-linear decision boundaries**. Infact the algorithm converges if the dataset is linearly separable, otherwise it does not terminate.

## Support Vector Machines (SVM)

Support Vector Machines (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. In classification, SVM finds the hyperplane that best separates the data into different classes while maximizing the margin between classes.

#### Linear SVM

In its simplest form, SVM constructs a linear decision boundary (hyperplane) that separates the data into two classes. The decision boundary is represented by the equation:

$$ f(x) = \text{sign}(\mathbf{w} \cdot \mathbf{x} + b) $$

where:
- $$ \mathbf{w} $$ is the weight vector,
- $$ \mathbf{x} $$ is the input feature vector,
- $$ b $$ is the bias term, and
- $$ \text{sign}(\cdot) $$ is the sign function.

#### Kernel Tricks

Kernel tricks extend SVM to handle non-linear decision boundaries by transforming the input features into a higher-dimensional space. The transformed data is then linearly separable, allowing SVM to find a hyperplane in the transformed space.

1. **Polynomial Kernel**:

   The polynomial kernel of degree $$ d $$ is defined as:

   $$ K(\mathbf{x}_i, \mathbf{x}_j) = (\mathbf{x}_i \cdot \mathbf{x}_j + c)^d $$

   where $$ c $$ is a constant term.

2. **Gaussian (RBF) Kernel**:

   The Gaussian (Radial Basis Function or RBF) kernel is defined as:

   $$ K(\mathbf{x}_i, \mathbf{x}_j) = \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2\sigma^2}\right) $$

   where $$ \sigma $$ is the kernel width parameter.

3. **Sigmoid Kernel**:

   The sigmoid kernel is defined as:

   $$ K(\mathbf{x}_i, \mathbf{x}_j) = \tanh(\alpha \mathbf{x}_i \cdot \mathbf{x}_j + c) $$

   where $$ \alpha $$ and $$ c $$ are constants.

Using these kernel functions, SVM can learn complex decision boundaries that are not possible with a linear classifier alone.

#### Soft Margin SVM

In situations where the data is not linearly separable or contains outliers, SVM can use a soft margin approach. The soft margin allows for misclassification of some data points by introducing a penalty term. This penalty term is controlled by the regularization parameter C, where a larger C value leads to a smaller margin and potentially fewer misclassifications.

#### Hyperparameters

1. **C Hyperparameter**:
    
    - The C hyperparameter controls the trade-off between maximizing the margin and minimizing the classification error.
    - A smaller C encourages a wider margin and tolerates more misclassifications, while a larger C penalizes misclassifications more heavily, leading to a narrower margin.
2. **Gamma Hyperparameter**:
    
    - The γ hyperparameter is specific to certain kernel functions, such as the Gaussian (RBF) kernel.
    - It defines the influence of a single training example, with low values meaning far reach (smoother decision boundary) and high values meaning closer reach (more complex decision boundary).

SVM is a versatile algorithm with various applications in classification and regression tasks. However, it may be sensitive to the choice of hyperparameters, such as the kernel type and regularization parameter, and may require tuning for optimal performance.


## K-nearest neighborhoods

k-Nearest Neighbor (k-NN) is a simple yet effective supervised learning algorithm used for classification and regression tasks. In k-NN classification, the class label of a data point is determined by the majority class among its k nearest neighbors in the feature space.

#### Algorithm Overview

1. **Training Phase**:
   - During the training phase, the algorithm simply stores the feature vectors and corresponding class labels of the training data.

2. **Prediction Phase**:
   - To predict the class label of a new data point:
     - Calculate the distance between the new data point and all training data points using a distance metric such as Euclidean distance.
     - Select the k nearest neighbors based on the calculated distances.
     - Assign the class label to the new data point based on the majority class among its k nearest neighbors.
     
#### Hyperparameters

1. **k Value**:
   - The value of k determines the number of neighbors to consider when making predictions.
   - A smaller value of k results in a more flexible model, potentially capturing more local variations in the data, but it may also be more sensitive to noise.
   - A larger value of k results in a smoother decision boundary, but it may also lead to oversmoothing and loss of important details.

2. **Distance Metric**:
   - The choice of distance metric, such as Euclidean distance, Manhattan distance, or Minkowski distance, affects how the distance between data points is calculated.
   - The appropriate distance metric depends on the characteristics of the data and the problem domain.

#### Pros and Cons

- **Pros**:
  - Simple and intuitive algorithm.
  - No training phase required.
  - Can handle multi-class classification and regression tasks.
  
- **Cons**:
  - Computationally expensive during prediction, especially with large training datasets.
  - Sensitive to the choice of k and the distance metric.
  - Performance may degrade in high-dimensional feature spaces due to the curse of dimensionality.

k-NN classifiers are particularly useful in situations where the decision boundary is complex and not easily captured by parametric models. However, careful selection of hyperparameters and preprocessing of the data are crucial for achieving good performance.

## Multi-Class Classification Strategies

#### One-Versus-One (OVO)

In the One-Versus-One (OVO) strategy, also known as pairwise classification, a separate binary classifier is trained for every pair of classes. For \( N \) classes, this results in $$\frac{N(N-1)}{2}$$ binary classifiers. During prediction, each binary classifier casts a vote, and the class with the most votes is selected as the final prediction.

- **Training Phase**:
  - Train $$ \frac{N(N-1)}{2} $$ binary classifiers, each distinguishing between one pair of classes.
  - Each classifier is trained on the subset of training data containing only the examples from the two classes it is responsible for discriminating.

- **Prediction Phase**:
  - For a new data point, apply each binary classifier to obtain $$ \frac{N(N-1)}{2} $$predictions.
  - The class with the most votes (the highest number of predicted occurrences) is selected as the final prediction.

#### One-Versus-Rest (OVR)

In the One-Versus-Rest (OVR) strategy, also known as one-against-all, a separate binary classifier is trained for each class, treating it as the positive class and combining the remaining classes into the negative class. During prediction, the class with the highest confidence score (output probability) from any of the classifiers is selected as the final prediction.

- **Training Phase**:
  - Train \( N \) binary classifiers, each treating one class as positive and the rest as negative.
  - Each classifier is trained on the subset of training data containing examples from the positive class and a random subset of examples from the negative class to balance the dataset.

- **Prediction Phase**:
  - For a new data point, apply each binary classifier to obtain \( N \) confidence scores.
  - The class associated with the highest confidence score is selected as the final prediction.

#### Comparison

- **Complexity**:
  - OVO requires training \( \frac{N(N-1)}{2} \) classifiers, which can be computationally expensive for large \( N \).
  - OVR requires training \( N \) classifiers, making it more scalable than OVO.

- **Decision Boundary**:
  - OVO constructs decision boundaries specific to each pair of classes, potentially leading to a more complex decision space.
  - OVR constructs decision boundaries for each class against the rest, resulting in simpler decision boundaries.

Both OVO and OVR are effective strategies for extending a binary classifier to multi-class classification tasks, each with its advantages and disadvantages. The choice between them depends on factors such as computational resources, dataset size, and the complexity of the decision boundaries.


## Ensemble methods
Ensemble classifiers train a set of base classifiers, utilizing their collective predictions to make the final decision. By aggregating the votes of these base classifiers, the ensemble method typically achieves superior performance compared to a single classifier.
Let's explore the dynamics of ensemble classifiers using a scenario involving 25 binary classifiers, each with an error rate of 0.35. The ensemble classifier's output is determined by the majority vote of these classifiers, requiring at least 13 agreeing predictions.

When the base classifiers are identical, the ensemble's error rate remains at 0.35. However, if the base classifiers possess the same error rate but are independent—meaning their errors are uncorrelated—the ensemble will only falter when the majority of classifiers are incorrect. This highlights the superior performance of ensemble classifiers over single classifiers.

Ensemble methods prove particularly effective under certain conditions:

1. **Independence of Base Classifiers**: When the base classifiers operate independently, the ensemble can leverage diverse perspectives, enhancing overall accuracy.
2. **Performance Beyond Random Chance**: Ensemble methods shine when the base classifiers individually outperform random guessing.

### Methods for dealing with  ensemble classifiers
1. These methods  are based on the **manipulation of training set** to build different base models:
	- **Resampling Techniques**: Data resampling strategies, like bagging, involve repeatedly drawing samples from the training set, either with or without replacement, to diversify the training process.
	- **Boosting**: This iterative technique adjusts the distribution of training examples, focusing the base classifiers' attention on instances that are hard to classify.
	- **Adaboost**: This algorithm assigns varying importance to each base classifier based on its error rate, dynamically adjusting their influence within the ensemble.
	
	In summary, by harnessing the independence of base classifiers and their individual strengths, ensemble methods can significantly outperform single classifiers, especially when employing sophisticated training set manipulation techniques like resampling and boosting.
2.  These methods are based on the manipulation of the **input features** in order to build different diversified base classifiers. 
3. These methods are based on the manipulation of the **class labels**. This method is particularly beneficial when dealing with a large number of classes. Each base classifier randomly divides the class labels into two subsets, A1 and A2, and re-labels the dataset accordingly. Binary classifiers are then trained to distinguish between the classes in each subset. During testing, when a subset is chosen, all included classes contribute to the final decision through a voting mechanism. The class with the highest score or most votes is declared the winner.

See these parts on the BOOK:
### Random forest
It is an ensemble classifier that uses decision trees as base classifiers. 

### Boosting
### Ada boost
### Gradient Boosting


# Regression
Da printare dall'altro obsidian

# The Data - Pre–processing and dissimilarities
Preprocessing consists in :
1. **AGGREGATION**: Combining two or more attributes (or objects) into a single attribute (or object). We do this for 
	1. *DATA REDUCTION*: the number of features decrease if we merget together two or more attributes
	2. *CHANGE OF SCALE*: if we aggregate cities in regions and countries we change the scale (NOTE in this case maybe we just map one attribute into another, so idk whether it is aggregation 100%)
	3. *MORE STABLE DATA*: when we aggregate we reduce the **variability** of the data.
	
  2. **SAMPLING**: We just sample some parts of the dataset (maybe there is the need to do it). The reasons could be :
	  1. obtaining the entire data set could be impossibile or too expensive
	  2. processing the entire data set could be too expensive or time consuming
	 Using a sample will work almost as well as using the entire data sets, if the sample is representative. A sample is representative if it has approximately the same property (of interest) as the original set of data.
	 There are different types of sampling:
	 1. Simple random: one sample is done randomly, following a uniform probability distribution
	 2. With replacement: more simple random samples are performed in an indipendent way (with replacement) 
	 3. Without replacement: more simple random samples are performed. The instances sampled are removed from the population.
	 4. Stratified: split data into several partitions according to *some criteria*, then draw random samples from each partition. It is fundamental that the dataset is split into subsets with homogeneous characteristics (the representativity must be guaranteed inside each subset)


# Clustering
## K-means
1. Suppose you were given the centroids: you could easily label all the instances in the dataset by assigning each of them to the cluster whose centroid is closest. 
2. If you were given all the instance labels, you could easily locate all the centroids by computing the mean of the instances for each cluster.
But you are given neither the labels nor the centroids, so how can you proceed? Well, just start by placing the centroids **randomly**.
The algorithm is **guaranteed to converge in a finite number of steps** (usually quite small), it will not oscillate forever ( This can be proven by pointing out that the mean squared distance between the instances and their closest centroid can only go down at each step).

### Complexity
The computational complexity of the algorithm is generally linear
with regards to the number of instances m, the number of clusters
k and the number of dimensions n. However, this is only true when
the data has a clustering structure. If it does not, then in the worst
case scenario the complexity can increase exponentially with the
number of instances. In practice, however, this rarely happens, and
K-Means is generally one of the fastest clustering **algorithms**.

Unfortunately, although the algorithm is guaranteed to converge, it may not converge
to the right solution (i.e., it may converge to a **local optimum**): this depends on the
centroid initialization.

**HOW TO DEAL WITH THIS**?
1. If you happen to know approximately where the centroids should be (e.g., if you ran
another clustering algorithm earlier), then you can set the init hyperparameter to a
NumPy array containing the list of centroids, and set n_init to 1
*kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)*
2. Another solution is to run the algorithm multiple times with different random initial‐
izations and keep the best solution. This is controlled by the n_init hyperparameter:
by default, it is equal to 10, which means that the whole algorithm described earlier
actually runs 10 times when you call fit(), and Scikit-Learn keeps the best solution.

But how exactly does it know which solution is the best? Well of course it uses a **per‐
formance metric**!
It is called the model’s inertia: this is the** **mean squared distance** between **each instance and its closest centroid**.
 model’s inertia is accessible via the *inertia_* instance variable
 >>> kmeans.inertia_
 >>> 211.59853725816856
The score() method returns the negative inertia. Why negative? Well, it is because a
predictor’s score() method must always respect the "great is better" rule.

### Finding the optimal number of clusters
You might be thinking that we could just run multiple Kmeans with different K values and pick the model with the lowest inertia, right? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much higher than for k=5 (which was 211.6), but with k=8, the inertia is just 119.1 (the inertia keeps getting lower and lower as the number of K increases).

The inertia is not a good performance metric when trying to choose k since it keeps get‐
ting lower as we increase k. Indeed, the more clusters there are, the closer each
instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s
plot the inertia as a function of k.![[c3.png]]\
As you can see, the inertia drops very quickly as we increase k up to 4, but then it
decreases much more slowly as we keep increasing k. This curve has roughly the
shape of an arm, and there is an “elbow” at k=4 so if we did not know better, it would
be a good choice: any lower value would be dramatic, while any higher value would
not help much, and we might just be splitting perfectly good clusters in half for no
good reason.

This technique for choosing the best value for the number of clusters is rather coarse.
A more precise approach (but also more computationally expensive) is to use the sil‐
houette score, which is the **mean silhouette coefficient** over **all the instances**.

An instan‐
ce’s silhouette coefficient is equal to **(b – a) / max(a, b)** where **a** is the **mean distance
to the other instances in the same cluster** (it is the **mean intra-cluster distance**), and **b**
is the **mean nearest-cluster distance**, that is the **mean distance** to the **instances of the
next closest cluster**. 
The silhouette coefficient can vary between -1 and +1: a coefficient close to
+1 means that the instance is well inside its own cluster and far from other clusters,
while a coefficient close to 0 means that it is close to a cluster boundary, and finally a
coefficient close to -1 means that the instance may have been assigned to the wrong
cluster.

\>>> from sklearn.metrics import silhouette_score
\>>> silhouette_score(X, kmeans.labels_)
0.65551764257282![[ss.png]]
As you can see, this visualization is much richer than the previous one: in particular,
although it confirms that k=4 is a very good choice, it also underlines the fact that
k=5 is quite good as well, and much better than k=6 or 7. This was not visible when
comparing inertias.

An even more informative visualization is obtained when you plot **every instance’s
silhouette coefficient**, sorted by the **cluster they are assigned** to and by **the value of the
coefficient**. This is called a **silhouette diagram**![[scplot.png]]
The vertical dashed lines represent the silhouette score for each number of clusters.
When most of the instances in a cluster have a lower coefficient than this score (i.e., if
many of the instances stop short of the dashed line, ending to the left of it), then the
cluster is rather bad since this means its instances are much too close to other clusters. 
1. We can see that when k=3 and when k=6, we get bad clusters.

2. But when k=4 or k=5, the clusters look pretty good – most instances extend beyond the dashed line, to the right and closer to 1.0. When k=4, the cluster at index 1 (the third from the top), is rather **big**. 

3. While when k=5, all clusters have similar sizes, so even though the overall silhouette score from k=4 is slightly greater than for k=5, it seems like a good idea to use k=5 to get clusters of similar sizes.
### Limits of KMeans
1. As we saw, it is necessary to run the algorithm several times to avoid sub-optimal sol‐
utions
2. You need to specify the number of clusters, which can be quite a hassle.
3. , K-Means does not behave very well when the clusters have **varying sizes**,
**different densities**, or **non-spherical shapes**.![[badkmeans.png]]
**IMPORTANT**: it is important to **scale the input features** before you run K-Means,
or else the **clusters may be very stretched**, and **K-Means will per‐
form poorly**. Scaling the features **does not guarantee** that all the
clusters will be nice and spherical, but it generally improves things.


### Hierarchical structure 
With simple likage bottom up e poi si CUTTA.
O con complete lickage o avg likage

1. Single linkage tends to generate clusters with larger diameters also at low levels 
2. Complete linkage tends to generate more compact clusters


1. The scaling is poor, due to the high complexity 
2. There isn’t a global objective function
3. The decision is always local and cannot be undone 
4. The dendrogram structure is of great help for the interpretation of the result 
5. Empirically, the result is frequently good
## DBSCAN
This algorithm defines clusters as continuous regions of high density. It is actually
quite simple:
	• For each instance, the algorithm counts how many instances are located within a
	small distance ε (epsilon) from it. This region is called the instance’s ε-
	neighborhood.
	• If an instance has at least min_samples instances in its ε-neighborhood (includ‐
	ing itself), then it is considered a **core instance**. In other words, core instances are
	those that are located in dense regions.
	• **All instances in the neighborhood of a core instance belong to the same cluster**.
	This may include other core instances, therefore a long sequence of neighboring
	core instances forms a single cluster
	• Any instance that is not a core instance and does not have one in its neighbor‐
	hood is considered an anomaly
This algorithm works well if all the clusters are **dense enough**, and they are **well sepa‐
rated by low-density regions**
```
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.05)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```
he labels of all the instances are now available in the labels_ instance variable:
\>>> dbscan.labels_
array(\[ 0, 2, -1, -1, 1, 0, 0, 0, ..., 3, 2, 3, 3, 4, 2, 6, 3])
Notice that some instances have a cluster index equal to -1: **this means that they are
considered as anomalies by the algorithm**.

The DBSCAN class does not have a predict() method,
although it has a fit_predict() method. In other words, **it cannot predict which
cluster a new instance belongs to**.
The Kmeans can instead no? You just compute the distances of the new value to the centroids and the value is assigned to  the label of the closest centroid.

In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any
number of clusters, of any shape, it is **robust to outliers**, and it has just **two hyper‐
parameters** (eps and min_samples). However, **if the density varies significantly across
the clusters**, it can be** **impossible for it to capture all the clusters properly**. Ha senso perche epsilon e min_samples magari funzionano per un cluster che ha una determinata densita' ma non per l'altro cluster, che ne ha un'altra capisci?

### Hyperparams, how to select them
As a rule of thumb, you can try minPoints  =2  ** D, the number of dimensions
A guess for $epsilon$ requires more effort, considering the distance of the k–nearest neighbour, with k  =  minPoints:

Consider the vector of the k-distances 
2. choose k 
3. for each point we compute the distance of its k–nearest neighbour and we sort the points for decreasing k–distance
![[Cattura 1.png]]
we can plot the k distance value when we increase the number of points in the dataset.
It decreases. 
When it starts decreasing and we observe a change in the slope we can do a gridsearch for the $epsilon$  in that area of k-distance.

### KDE

DENCLUE algorithm 
1. Derive a density function for the space occupied by the data points 
2. Identify the points that are local maxima 
3. Associate each point with a density attractor by moving in the directions of maximum increase in density 
4. Define clusters consisting of points associated with a particular density attractor 
5. Discard clusters whose density attractor has a density less than a user–specified threshold ξ
6. Combine clusters that are connected by a path of points that all have a density of ξ or higher
#### PROS AND CONS
1. It has a strong theoretical foundation on statistics 
	1. precise computation of density 
	2. DBSCAN is a special case of DENCLUE where the influence is a step function
2. Good at dealing with noise and clusters of different shapes and sizes 
3. expensive computation O(N2 )qcan be optimized with approximated grid based computation 
4. Troubles with high dimensional data and clusters with different densities
### Model based clustering: GAUSSIAN MIXTURE
