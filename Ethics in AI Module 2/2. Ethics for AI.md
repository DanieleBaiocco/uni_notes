# Individual and social costs of AI & Big Data applications
In some cases and domain, AI & Big Data applications—even when accurate and unbiased— may have individual and social costs that outweigh their advantages.

Consider, for instance, systems that are able to recognise sexual orientation, or criminal tendencies from the faces of persons. Should we just ask whether these systems provide reliable assessments, or **should we rather ask whether they should be built at all?**

The key aspect of ML systems is the ability to engage in **differential inference**: different combinations of *predictor-values* are correlated to *different predictions*.

**The general problem of social scoring and differential treatment** is that
a new dynamic of **stereotyping and differentiation** takes place:
1. The individuals whose data support the same prediction, will be considered and treated in the same way.
2. The individuals whose data support different predictions, will be considered and treated differently
## Problemi nell'utilizzo dell'ML nel predirre la salute di un umano
This provision has become hugely relevant in the context of the Coronavirus disease 2019 (COVID-19) epidemics. In particular a vast debate has been raised by development of applications for tracing contacts.
Such processing should be viewed as legittimate as long as it effectively contributes to limit the diffusion and the harmfulness of the epidemics, assuming that the privacy and data protection risks are proportionate to the expected benefit, and that appropriate mitigation measures are applied.

C'e' pero' un piccolo problema in questa questione: 
se applico ML per predirre la salute di un paziente va bene,  se poi questa informazione viene utilizzata magari nel contesto dei **prestiti** per **non cedere** un prestito a una persona che, secondo il modello di ML, morira' tra poco, allora c'e' un grosso problema. 
In sto modo infatti si avrebbe un mondo in cui se  una persona sta male, viene ulteriormente svantaggiata anche in quest'altro contesto, causando emarginazione e maggiori sofferenze.

Immagina ancora cosa significa applicare queste predizioni **sulla salute**, invece nel contesto delle *assunzioni lavorative*. Sarebbe tremendo, in quanto questa persona sarebbe tagliata fuori **addirittura anche dal contesto lavorativo**. 

## Problemi della Discriminazione di Prezzo
Un'altra cosa che potrebbe avvenire e' la *discriminazione di prezzo*. Quindi il fenomeno per cui verrebbero consigliati diversi prezzi di prodotti a seconda della loro **voglia e disponibilita' nel comprare un determinato prodotto**. Ad esempio alle donne potrebbe essere mostrato un prezzo piu' alto per le scarpe, avendo il modello di ML imparato che alle donne piacciono motlo, mentre ai maschi potrebbe esser mostrato un prezzo basso per le stesse scarpe e un prezzo alto magari per delle *griglie*.![[ethics23.png]]
Questo porta a qualcosa di poco etico, in quanto certi individui **verrebbero privati di certe opportunita'** solo perche' appartengono a un determinato gruppo imparato dal modello.

## I sistemi di AI sono meglio degli umani  nel valutarci?
In tanti *domains* si ha che **predizioni automatiche** date da una AI non solo sono piu' **economiche** rispetto a avere un gruppo di persone che fa quel tipo di scelte, ma spesso sono anche **piu' precise e imparziali** rispetto agli umani:
1. L'AI puo' evitare i tipici errori della **psicologia umana** come essere over-confidenti, avere dei bias e evitare l'**inabilita' da parte degli umani di processare dati statistici**, oltre all'evitare **i pregiudizi umani** (riguardo ad esempio al sesso, all'etnia).
2. In molti campi si e' proprio visto che sistemi algoritmici (come AI systems) hanno spesso performato meglio rispetto a esperti umani

I problemi dell'umano nel valutare sono i seguenti:
* Over-confidenza: quando gli umani hanno piu' confidenza del dovuto nelle proprie abilita', nel fatto di sapere e saper agire
* Avversione verso la perdita: gli umani tendono a preferire la mancanza di perdite, rifiutano la possibilita' di una perdita, anche se questa potrebbe poi portare a un guadagno maggiore nel  lungo tempo
* Ancoraggio: le decisioni degli individui sono **influenzate** da un punto di riferimento particolare, anche detto ancora, AD esempio un individuo potrebbe essere invogliato a comprare una macchina se questa e' messa vicina a una macchina di un modello molto piu' costoso (quindi l'individuo vede un'offerta, basandosi sul reference point, sull'ancora rappresentata dall'altra macchina piu' costosa)
* Bias di conferma: gli individui tendono a fare scelte perpetrando le loro idee e le loro credenze (che sono spesso biased e legate al proprio penisero)


Ci sono stati casi pero' di AI che erano *discriminatorie*. In pochi casi c'e' stata una discriminazione esplicita, chiamata *disparate treatment*, in cui l'AI basava i suoi outcomes su features proibite come la razza, l'etnia, il gender.
Si tende piu' a associare un outcome a una razza se il dataset e' fatto per cui la razza e' legata a un certo tipo di outcome **causa discriminazione razziale, causa contesto sociale INGIUSTO**.

**IMPORTANTE** infatti precisare che:
Dei sistemi basati su *supervised learning* possono infatti esser trainati su *giudizi umani passati* e quindi possono *riprodurre* i punti di forza ma soprattutto quelli di **debolezza** degli umani che hanno dato quel tipo di giudizio, che includono ovviamente la propensione di alcuni di esser pregiudizievoli verso determinate etnie,ecc...

## Pregiudizi nel training set
Il pregiudizio e la discriminazione all'interno del dataset puo' anche essere presente nonostante le *prohibit features* vengano escluse (quindi features riguardanti il sesso, la razza, la regione di appartenenza, ecc...).
Infatti basta che ci siano **correlazioni**, anche NON dirette, tra features discriminatorie e predizioni del training set per avere discriminazioni. 
Ad esempio si potrebbe avere un *dataset* in cui NON e' presente la feature della razza a tempo di training MA che e' riferito a assunzioni fatte da un manager con grossi pregiudizi. Questo manager NON ha mai assunto persone negre. Queste persone abitavano (e continuano a abitare) nello stesso quartiere. 
La feature del quartiere di provenienza e' pero' presente nel dataset utilizzato per il training. Di conseguenza quando si faranno predictions, dato l'input, si sara' discriminatori verso il quartiere di provenienza (si tendera' a non assumere persone provenienti da quel quartiere, perche' nel dataset persone provenienti da li' non vengono mai assunte) e di riflesso lo si sara' verso le persone nere , in quanto maggiormente risiedono in quel quartiere.

## Biases del sistema verso determinati gruppi
Ci sono casi in cui un training set e' biased contro un particolare gruppo di persone, in quanto l'outcome che viene predictato e' approssimato attraverso un **proxy**, quindi una sola feature che risulta poi essere discriminatoria.

Per fare un esempio:
Immagina che si utilizzasse un'AI per dare uno score a ogni persona, che indica, dati degli inputs, quanto quella persona e' **produttiva**. Mettiamo che questa prediction sia solo legata al **numero di ore lavorate in ufficio**. Il dataset e' ovviamente legato al passato, e si avra' che le donne tenderanno a avere uno score piu' basso. Cio' e' dovuto dal fatto che le donne lavorano in generale meno ore causa problemi legati al travaglio, alla gestazione ecc...
Quindi il modello predicterebbe che alle donne e' giusto associare uno score piu' basso, in quanto sono meno performanti rispetto agli uomini.

## Biases del sistema presenti nelle input features
A volte ci sono casi in cui il sistema performa in maniera discriminatoria perche' ha come input feature una categorical feature magari che e' **SEMPRE** settata a False per membri di un determinato gruppo, mentre per altri gruppi varia (o e' per la maggioranza True). La feature potrebbe essere *e' stato iscritto a una scuola superiore*. Ovviamente e' piu' probabile che per persone di colore, poco abbienti e con condizioni sociali precarie, questa feature sia settata a False, ma in questo modo il dataset rispecchia una discriminazione che avviene nel mondo reale. La prediction non dovrebbe essere basata su una feature su cui le persone di colore non hanno avuto scelta magari (tanti di loro avrebbero voluto andare a scuola ma per poche disponibilita' economiche non e' stato possibile).

## Biases dovuti a datasets che non rispecchiano la composizione statistica della popolazione
Metti che c'e' un'AI che predicta la possibilita' di pagare una cauzione date delle features su una persona. E metti che una delle features in input sia *quanti reati hai commesso*. Per le persone nere questo numero sara' piu' alto, NON solo per motivi magari legati a disagio sociale, situazioni precarie ecc..., ma anche a causa del fatto che **si tende, soprattutto in America** a fare **controlli piu' stringenti a persone nere, mentre per quanto riguarda i bianchi, anche se hanno commesso qualcosa, magari non si scopre niente proprio perche' i controlli per loro invece solo piu' blandi**. Conta che in America la probabilita' che un nero venga fermato per dei controlli dalla polizia e' 8 volte piu' alta rispetto ai bianchi.

C'e' anche da pensare al fatto che membri di un gruppo potrebbero esser discriminati dall'AI causa fatto che nel dataset la porzione di dati riferita a loro sia molto esigua e quindi non si riesca a imparare per bene a predictare per quei gruppi, creando non affidabilita' nella predizione dell'AI e quindi possibile ingiustizia.

## Considerazioni su come affrontare l'ingiustizia di sistemi AI che performano decision-making 
Si e' visto che spesso questi sistemi AI sono **comunque**, nonstante **tutto**, piu' controllabili delle decisioni prese dagli umani. Le discriminazioni possono difatti essere identificate con precisione e questi outcome discriminatori possono esser migliorati per evitare outcomes ingiusti.

Un'altra cosa importante da dire e' che: un algoritmo biased puo' comunque essere **piu' giusto** rispetto a un umano che prende una decisione nello stesso contesto.

La cosa migliore da fare consiste nell'integrare outcomes dell'AI con revisione da parte degli umani. Quindi dando la possibilita' agli individui vittime di potenziali discriminazioni da parte dell'AI di richiedere una **revisione umana** della predizione fatta dal sistema.

**IL GDPR, prendendo in considerazione tutte queste problematiche, ha introdotto dei constraints**:
1. Hanno introdotto il bisogno di far leggi per gestire e limitare l'utilizzo di *dati personali* degli utenti
2. Hanno introdotto obblighi legati alla trasparenza di un sistema AI
3. Hanno introdotto limitazioni riguardanti il *profiling* e riguardanti sistemi *di automated decision making*
4. Hanno introdotto dei requisiti rugardanti l'anonimizzare i dati

## Social Empowerment
L'utilizzo della AI e' utilizzato gia' in domini in cui c'e' gia' una grossa differenza di potere (persone gia' ricche che aumenterebbero la disparita' e hanno ora la possibilita' di far ancor piu' il cazzo che gli pare).
Serve la **societa'** per scovare *abusi, informare il pubblico, attivare delle contro-misure*. E l'AI puo' essere utile appunto a cittadini di questa societa' per sviluppare sistemi che tutelino e proteggano l'utente dalle minacce di altri sistemi AI fatti da gente cattiva magari (come detto sopra).

Ad esempio, i cittadini hanno nel tempo sviluppato degli strumetni come *ad-block systems, anti-spam softwares, anti-phishing techniques, ecc...* che possano controbilanciare queste cose.
Ci sono stati due progetti che seguono qeusta scia che sono:
1. Claudiette: veditelo
2. PDA/CDA che sta per Privacy digital assistants/consumer digital
assistants: sarebbe una proposta per estrarre autocamticamente, categorizzare e riassumere informazioni legate a *documenti sulla privacy*, in modo da assistere l'utente nel processarli e capirne i contenuti. Il motivo e' prevenire la collezione di dati personali eccessivi, non voluti e illegali , inoltre to protect users from
manipulation and fraud, provide them with awareness of fake and
untrustworthy information, and facilitate their escape from “filter
bubbles”.
