Ci sono delle questioni etiche grosse dietro le AV.
We start by saying that a self-driving Uber killed a woman that crossed the road.
Given that, now we talk about PROS and CONS:
PROS:
1. in teoria si ha un ridotto numero di incidenti
2. migliorerebbe la mobilita' per persone con disabilita' (cieche) e bambini
3. il trasporto verrebbe visto come un servizio e non come un'attivita' in cui UNO deve guidare e deve stare attento
CONS/OBSTACLES:
1. Problem of liablility: chi e' responsabile nel caso in cui avviene un incidente?
2. Problema di hacking e di sicurezza: si puo' hackerare l'AV 
3. Problemi legati a dilemmi etici che non sono ancora risolti
LE DOMANDE CHE MI PONGO SONO:
![[Pasted image 20240607111223.png]]
AV are veichles that are capable of sensing their environment and navigating without human input.

**ma cos'e' la LIABILITY**? Beh e' ![[Pasted image 20240607111344.png]]
Nel dominio legale ci sono 3 tipi di liability:
Il primo e' legato a un CRIMINE, il secondo e' legato a un TORTO o la VIOLAZIONE di un contratto, la TERZA alla violazione di regole amministrative o regolazioni (DI QUESTA NON PARLERO' PERCHE' non puo' riguardare le AV).
Nota che si possono avere piu' capi di accusa legati alla liablity per una stessa cosa successa.

![[Pasted image 20240607111613.png]]

------------------------
Inserisco un attimo la differenza tra persona naturale e persona legale
![[Pasted image 20240607111930.png]]
--------------------------------------------------------------------------------------------------------------------
# CRIMINAL LIABILITY
Per iniziare la **criminal liability** si estende sia a *natural persons che a legal persons*. Inoltre, per avere criminal liability in un fatto avvenuto, deve esser stata compiuta **un'azione o un'omissione** che violano la legislazione nazionale sui crimini(esempio: o aver sparato  a uno (azione) o esser fuggito dopo aver preso sotto qualcuno (omissione)) (le due cose possono avvenire anche una dopo l'altra). Questo si chiama l'**actus reus**.
SI va poi a vedere la **mens rea** ovvero il motivo per cui tale azione o omissione e' stata compiuta. I motivi possono esser diversi e sono: INTENZIONE, NEGLIGENZA, INCOSCIENZA (quindi o VOLEVO farlo, volevo uccidere quella persona comunque, o e'  stato un errore, o ecc...).

```
ESEMPIOZZO:
SE QUALCUNO CON UNA MACCHINA, MENTRE GUIDA A ALTA VELOCITA, COLPISCE UN GRUPPO DI PASSANTI (quindi compie un'AZIONE) per sbaglio (quindi la mens rea/lo stato mentale di sto tipo e' di NEGLIGENZA), potrebbe essere CRIMINALLY LIABLE (responsabile a livello criminale) per aver causato la morte dei passanti (quindi OMICIDIO).

Se poi il conducente INTENZIONALMENTE non da' assistenza ai passanti presi (quindi si ha una OMISSIONE con mens rea di INTENZIONE) allora dovra' affrontare altri capi d'accusa come omicidio E omissione di soccorso.
```
La regola da rispettare era il **limite di velocita'**, che serviva a tutelare da situazioni di questo tipo.

# CIVIL LIABILITY
Abbiamo che:
![[Pasted image 20240607113332.png]]
Quello che interessa a me e' il **PRODUCT LIABILITY**.
Ogni prodotto nel mercato e' infatti legato a una liability.
Le condizioni che devono esserci per attivare una product liablility sono:
![[Pasted image 20240607113529.png]]
Quindi quando quel prodotto ha dei difetti di design, di manifattura, o difetti di allarme e quando quel prodotto per colpa di questi difetti causa dei datti.
NELLO SPECIFICO I DIFETTI SONO SPIEGATI QUA SOTTO:
![[Pasted image 20240607113650.png]]

# LIABILITY E AUTOMAZIONE
IN UN QUALCOSA DI AUTOMATICO AVVENGONO I SEGUENTI STEPS:
![[Pasted image 20240607114026.png]]
Nota che in un contesto automatico  NON per forza tutti gli steps sono automatici, alcuni di essi hanno al loro interno l'intervento umano talvolta.
![[Pasted image 20240607114327.png]]
Infatti PER OGNUNO DI QUESTI STEPS, come e' visibile dalla tabella, ci sono DIFFERENTI LIVELLI DI AUTOMAZIONE (dall'alto verso il basso si va dal manuale piu' ostinato fino alla completa automazione).
Al livello 0 per ogni step (A0, B0, ecc..) ho che si fa tutto manualmente.
Un livello intermedio di automazione nell'information acquisition per le AV e' MENO AFFIDABILE solitamente rispetto a una automazione completa. In sto step infatti HO POCO TEMPO PER ACQUISIRE LE INFORMAZIONI, quindi un'interazione umana sarebbe solo un problema e rallenterebbe l'acquisizione delle informazioni. Lo stesso vale per gli altri steps nel caso dell'AV.

Nell'AV ho i seguenti livelli di automazione:
![[Pasted image 20240607115228.png]]
Le macchine di adesso sono a Livello 0, mostrano solo dei segnali luminosi tramite sensori che notificano cosa non va.

![[Pasted image 20240607115402.png]]
NEL CASO DI FULL AUTOMATION HO CHE SI HA **PIU' ALTA LIABILITY PER IL MANUFACTURER**
CON UNA MEDIUM AUTOMATION HO UNA **LIABILITY CHE E' CONDIVISA** (qui si va a vedere di chi e' stata effettivamente la colpa, se di un difetto nella parte di acquisizione automatica delle informazioni o negli steps successivi o se dell'umano che non era attento o ha preso una decisione sbagliata)
CON UNA LOW LEVEL AUTOMATION HO UNA **LIABILITY PIU' ALTA PER LA PERSONA CHE GUIDA**

# COLLISIONI INEVITABILI

## THE MORAL MACHINE
C'e' stato sto forum in cui si e' chiesto a persone di scegliere tra delle scelte BINARIE in situazioni simulate di incidenti inevitabili. I partecipanti hanno dovuto rispondere a 13 domande. Le opzioni in ogni domanda sono :  *continua dritto* o *sterza di lato*. DUE DEGLI SCENARI PROPOSTI SONO I SEGUENTI:
![[Pasted image 20240607121259.png]]
Nel caso a) il numero di persone che muore e' IN ENTRAMBE le opzioni *continua dritto* o *sterza di lato* UNO. Nel caso b) invece se sterzo muoio solo io, mentre se continuo dritto muoiono 3 passanti.
![[Pasted image 20240607121447.png]]
![[Pasted image 20240607121517.png]]
INOLTRE **il fatto di programmare questo approccio UTILITARIO** nelle macchine da parte del programmatore, senza la possibilita' di cambiare questo approccio, E' STATO VISTO DAI PARTECIPANTI ALL'ESPERIMENTO COME QUALCOSA DI SBAGLIATO.
## QUANDO IL GUIDATORE E' UN UMANO
A livello legale si ha che IN ENTRAMBI GLI SCENARI a) e b) si puo' applicare lo **STATE OF NECESSITY**. Secondo questa legge, se io uccido i passanti, pur di salvarmi la vita, sapendo che sterzando sarei morto, io non posso essere perseguibile penalmente. A meno che ovviamente non andavo oltre il limite di velocita' eheh.

Per applicare questa **STATE OF NECESSITY** ho bisogno di due condizioni che devono esser vere:
1. ci deve essere un danno serio ai danni di qualcuno per colpa di un *offender* che **non sia stato VOLONTARIO e che fosse INEVITABILE**
2. cio' che l'*offender* ha causato deve essere proporzionale al PERICOLO (quindi piu' alto e' il pericolo piu' ) NON CHIARO NON CHIARO

PER SPIEGARE MEGLIO LO STATE OF NECESSITY
![[Pasted image 20240607122600.png]]
Lo state of necessity si puo' quindi applicare nel caso in cui un guidatore prende sotto dei civili/passanti.
NEL NOSTRO SCENARIO quindi NON HO CRIMINAL LIABILITY se decido di continuare dritto, cio' e' vero sia nello scenario a) che in quello b).
Cio' ovviamente non vale per la **civil liability**, che invece si applica nel caso di danni causati. In sto caso l'assicurazione entra in gioco per pagare questi danni.

C'E' PERO' QUALCOSA DI DIVERSO SE PRENDO IN CONSIDERAZIONE QUESTI 3 CASI:
![[Pasted image 20240607123350.png]]
C'e' un nuovo caso, IL PRIMO. Nel primo caso IO CONDUCENTE NON MUOIO. Quindi non si puo' applicare  la STATE OF NECESSITY perche' la mia vita non e' in pericolo. In questo caso c'e' una scelta morale da fare. Si puo' scegliere di agire secondo OMISSION e NON prendere decisioni quindi (lasciando che si realizzi il naturale corso degli eventi), o di agire secondo un'AZIONE, quindi sterzando e uccidendo il tipo (che tra l'altro era in una situazione **sicura** perche' non stava attraversando e sto poro cristo deve pure morire che cazzo).

ENTRAMBE LE SCELTE SONO OK nel primo caso. NEGLI ALTRI DUE posso apllicare la state of necessity e bona.
# QUANDO IL VEICOLO E' PRERPOGRAMMATO
In sto caso la scelta in situazioni di questo tipo e' GIA' STATA PRESA DA UN PROGRAMMATORE in fase di progettazione della macchina.
La persona quindi che ha preso questa SCELTA NON E' PRESENTE mentre l'incidente sta per avvenire. Questa persona ha il potere, programmando, di decidere chi rimane in vita e chi muore in situazioni di questo tipo.

SE RIPRENDO IN ESEMPIO I SEGUENTI SCENARI:
![[Pasted image 20240607123350.png]]
NEL PRIMO SCENARIO il programmatore non sarebbe giustificato quando sceglie di programmare una AV in modo che rimanga sulla strada. In sto caso non e' possiblie fare la distinzione di prima tra la scelta di OMISSIONE (assecondare il naturale corso degli eventi) e quella di AZIONE (sterzare e cambiare il naturale corso degli eventi) perche' il programmatore NON E' LI' quindi non si applica sta cosa. CIO' E' DOVUTO AL FATTO CHE IL PROGRAMMATORE NON E' LI', E' UN OSSERVATORE ESTERNO.

NEL SECONDO SCENARIO Ã¨ molto dubbio che preprogrammare l'auto **per andare dritto** che **per sterzare** sarebbe legalmente accettabile: in entrambi i casi, **il programmatore sceglierebbe arbitrariamente tra due vite.** 

NEL TERZO SCENARIO seems that preprogramming the car to continue on its trajectory, causing the death of a higher number of people **could not be morally or legally justified**: it would amount to an a**rbitrary choice to kill many rather than one**. 

Quindi il PROGRAMMATORE non e' legalmente perseguibile soltanto nei casi a) e c) quando sceglie di sterzare, quindi quando segue un approccio UTILITARIO (utilitarian approach) che minimizza le perdite.
Nel caso b) sara' sempre legalmente perseguibile. Qui non si puo' applicare un'utilitarian decision.

# THE ETHICAL KNOB
Per risolvere questi problemi, o comunque provare a rendere il tutto molto piu' customizzabile si e' creata sta Ethical Knob.
![[Pasted image 20240607125156.png]]
QUINDI NEI DUE VECCHI SCENARI a) e b) storici visti prima avrei che
![[Pasted image 20240607125317.png]]
In sto modo a livello legale posso sciogliere delle questioni. Ora posso deresponsabilizzare il programmatore e dare la responsabilita' al GUIDATORE. Pero' visto che questo comportamento viene scelto all'acquisto della macchina, o comunque settato prima di partire con la macchina, **NON si puo' applicare il ragionamento visto sopra** tra la scelta di OMISSIONE (assecondare il naturale corso degli eventi) e quella di AZIONE (sterzare e cambiare il naturale corso degli eventi).
CON STO KNOB avrei che ![[Pasted image 20240607125844.png]]

# THE ETHICAL KNOB 2.0
SE PERO' TENIAMO IN CONSIDERAZIONE ANCHE QUAL E' LA PROBABILITA' DI MORTE DEL CONDUCENTE (la probabilita' di pericolo) RISPETTO A QUELLA DEI PASSANTI allora sto KNOB puo' diventare piu' complicato e anche piu' utile diciamo.
![[Pasted image 20240607130214.png]]
Quindi ora il KNOB 2.0 ti fa scegliere quanto la vita di te conducente sia piu' importante rispetto alla vita dei civili che potrai mettere sotto.
E si tengono in considerazione anche le probabilita' di farsi male calcolate a priori poco prima che il fattaccio avvenga.
Qui anche se la vita del guidatore e' PIU' IMPORTANTE di quella del passante, comunque il passante ha una probabilita' di morte molto piu' alta, quindi l'AV sceglie di mettere il guidatore in una situazione di rischio e di salvare il passante.
![[Pasted image 20240607130439.png]]
In questo altro esempio, il guidatore ha messo un'importanza di se stesso cosi alta che alla fine l'AV prende sotto il passante e lo uccide. IN STO CASO puo' il guidatore  essere perseguibile penalmente perche' ha messo un valore TROPPO alto della sua vita rispetto a quella degli altri?
![[Pasted image 20240607130501.png]]
BEH sicuramente servirebbero a livello legale delle leggi che mettessero dei LIMITI all'importanza data alla propria vita. It would be up to every **legal system** to **determine the threshold for acceptable selfishness**.
NON GUARDARE LE LINEE DIAGONALI CHE SONO FUORVIANTI:
![[Pasted image 20240607131334.png]]
In verita' il grafico e' molto chiaro.
Data una probabilita' di 0.5 di morire del conducente se sterza, e 0,9 di morire dei passanti se presi dalla macchina si ha che alla fine l'expected disutility di sterzare e' piu' BASSA rispetto a quella di andare DRITTO, anche se il conducente ha settato la sua vita a un VALORE PIU' ALTO RISPETTO ALLA VITA DEI passanti.
![[Pasted image 20240607131426.png]]
IL PROBLEMA E' QUELLO VISTO PRIMA  ovvero che:
the importance that the passenger attributes to its own life is so high that even a small risk of death for the passenger determines a disutility that is higher than the disutility corresponding to a very high risk for the pedestrian
e la soluzione potrebbe essere quella proposta sopra ovvero
`BEH sicuramente servirebbero a livello legale delle leggi che mettessero dei LIMITI all'importanza data alla propria vita. It would be up to every **legal system** to **determine the threshold for acceptable selfishness**.`

