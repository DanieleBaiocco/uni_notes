![[Pasted image 20240607151548.png]]
# ARTICOLO 2 DELL'AI ACT
![[Pasted image 20240607151631.png]]\
Sto ai act si applica anche a altri attori legati all'AI
![[Pasted image 20240607151641.png]]
L'AI ACT e' un documento **COMPLESSO**. Ci sono grossi OVERLAPS tra articoli scritti in AI ACT con quelli gia' presenti nel GDPR. Ci sono anche grosse **contraddizioni** con quello che c'e' scritto nel GDPR.

NELL'AI ACT e' presente una definizione di AI tale che:
![[Pasted image 20240607152358.png]]
Ci sono due problemi con questa definizione che e' stata data:
1. Manca la parte della ROBOTICA
2. Non ha senso listare delle tecniche per fare Machine Learning (come Bayesian estimation) che potrebbero cambiare nei prossimi anni. Potrebbero nascerne di nuove e a quel punto questa definizione NON andrebbe piu' bene
Difatti questa definizione e' stata molto criticata dalla comunita'.

Quelli dell'AI ACT si sono chiesti: "Quando un sistema AUTOMATICO puo' essere considerato essere AI?"
La risposta che hanno dato e' nell'**ARTICOLO 3 (1)**, basandosi sul concetto di AUTONOMIA.
C'e' il termine **adaptiveness**, sono sistemi che si ADATTANO. C'e' anche il termine **autonomy**, quindi sono AUTONOMI. Possono anche influenzare lo spazio virtuale o fisico in cui operano (quindi si include anche la ROBOTICA stavolta). Inoltre fanno inferenze.
![[Pasted image 20240607152915.png]]
Questa definizione di AI ha molto piu' senso della definizione che avevano dato all'inizio mostrata sopra. 

Le idee core dietro all'AI ACT sono la **SICUREZZA** e la **PREVENZIONE DEL RISCHIO**.

Secondo l'AI ACT ogni volta quindi che viene prodotto un prodotto, c'e' da fare un **risk assessment** legato a quel  prodotto in relazione ai **DIRITTI FONDAMENTALI DELL'UOMO**.
Si fa una cosa simile generalmente quando esce un nuovo prodotto fisico, che pero' riguarda la sicurezza legata a quel prodotto chiamato **PRODUCT SAFETY APPROACH**, in cui si fa un **risk assessment** e si testa se quel prodotto puo' essere fisicamente dannoso in modo dettagliato.

Il problema e' che nel caso di prodotto legato all'AI, NON SEMPRE SI SA come questo impattera' e violera' i DIRITTI FONDAMENTALI dell'uomo. Si puo' prevedere ma non e' cosi facile come magari lo e' per i PRODOTTI materiali diciamo. 

# A risk-based approach
Nell'AI ACT si e' quindi provato a fare un **risk assessment** per tipi di prodotti legati all'AI, in base a quanti diritti fondamentali venivano calpestati e in quale estensione (extent).
![[Pasted image 20240607155353.png]]
Di base si prende in considerazione il dominio dell'applicazione dell'AI per fare risk assessment. In base al dominio si capisce quanti fundamental rights sono presi in considerazione e sono potenzialmente a rischio.

MA COS'E' IL RISCHIO? SECONDO L'ARTICOLO 3 (2)
![[Pasted image 20240607162011.png]]
TIPO il rischio con AV e' INVESTIRE passanti in aree urbane, perche' e' una combinazione di probabilita' di colpirli e la severita' del danno (che e' solitamente alto quando si parla di investire con la macchina qualcuno).
IL PROBLEMA in sto caso e' che spesso **la probabilita' dell'occorrenza di un'HARMFUL event** non e' sempre accessibile.
Inoltre PROPRIO IL TIPO DI EVENTO HARMFUL non e' identificabile a priori, perche' stiamo parlando DI VIOLAZIONE DI DIRITTI FONDAMENTALI. Questa violazione non e' DEFINIBILE/CALCOLABILE A PRIORI (riguarda le persone viste come SINGOLI, oguno potrebbe sentirsi violato nei diritti fondamentali e i casi possibili SONO troppi per riuscire a comprenderli e prevederli prima).

CALCOLO il rischio legato a un evento come L'EXPECTED COST di quell'evento. Ho che un evento HARMFUL (DANNOSO) ha un rischio ALTO quando il suo EXPECTED COST e' alto.

L'expected cost e' l'*event disutility* (quindi il danno da esso comportato, che e' legato comunque ai FUNDAMENTAL RIGHTS, quanto vengono violati) moltiplicato per *la probabilita' che quell'evento avvenga*.
![[Pasted image 20240607163536.png]]
![[Pasted image 20240607163607.png]]
UN RISCHIO E' ACCETTABILE quando i BENEFITS (L'EXPECTED BENEFIT) sono MAGGIORI dei costi (dell'EXPECTED COST).
INOLTRE IL RISCHIO DEVE ESSERE LIMITATO attraverso delle MISURE DI MITIGAZIONE che abbassino il rischio di base. Pero' c'e' da avere pure che I VANTAGGI che si ottengono applicando queste misure di mitigazione che abbassano il rischio devono esser MAGGIORI del **costo a livello monetario mi vien da dire**  di queste misure di mitigazione.
INFATTI IL PROBLEMA E' CHE :
![[Pasted image 20240607164147.png]]

# PRATICHE PROIBITE dall'AI ACT
Le pratiche proibite sono secondo L'ai act:
![[Pasted image 20240607164401.png]]
I PRIMI DUE PUNTI SONO SPIEGATI MEGLIO QUI SOTTO, in un'appendice dell'AI ACT

![[Pasted image 20240607164743.png]]
# AI SYSTEMS CON HIGH RISK
QUELLE CHE RIENTRANO IN QUESTO GRUPPO SONO:
1. Quelli presenti in ANEX 2, SOLO QUANDO SONO  APPLICATI A health e safety DOMAINS.
2. Quelli presenti in ANEX 3, che sono i piu' rilevanti
DALLE SLIDES HO L'ELENCO SCRITTO COSI:
![[Pasted image 20240607165542.png]]
IN anex 2 ho i seguenti sistemi:
![[Pasted image 20240607165250.png]]
Questi sono pericolosi solo se sono usati in HEALTH e SAFETY domains

Quelli che sono invece problematici sono i sistemi riguardanti i seguenti campi/settori:
![[Pasted image 20240607165339.png]]

CI SONO DUE COSE RIMASTE FUORI, legate AL RISCHIO:
1. Autonomous weapons
2. robots legati alla SPACE EXPLORATION (che non si sa come fanno a essere rischiosi riguardo ai fundamental wrights. Magari lo sono sotto il punto di vista della safety)



![[Pasted image 20240607170054.png]]
![[Pasted image 20240607170145.png]]
## COME REGOLARE HIGH RISK AI SYSTEMS
L'AI ACT dice che per HIGH RISK AI SYSTEMS C'E' L'OBBLIGO DI FARE/AVERE LE SEGUENTI COSE
![[Pasted image 20240607170425.png]]
![[Pasted image 20240607170522.png]]

## FUNDATION MODELS
la prof ora parla dei FUNDATION MODELS che ho gia visto nella parte riferita AL GENERATIVE AI.
AVEVO GIA' VISTO NEL DETTAGLIO IN QUELLA PAGINA DI OBSIDIAN in cosa consistono e come sono regolamentati nell'AI ACT. Ecco l'immagine presa da li'
![[Pasted image 20240607110258.png]]
CI sono poi altre cose riguardanti GLI OBBLIGHI di questi fundation models, che non erano presenti nella pagina di obsidiano 6. GENERATIVE AI che sono invece presenti qui che sono:
![[Pasted image 20240607170940.png]]
![[Pasted image 20240607173543.png]]

![[Pasted image 20240607173648.png]]
![[Pasted image 20240607173946.png]]
![[Pasted image 20240607173914.png]]
NELL'AI ACT C'E' SCRITTO CHE OGNI STATO DEVE AVERE LE SUE AUTORITA' CHE FACCIANO IN MODO CHE L'AI ACT (*this Regulation* nel testo qua sotto) SIA RISPETTATO  IN QUEL PAESE IN PARTICOLARE.
![[Pasted image 20240607174226.png]]
LA BOARD fa da tramite, da assistene alla Commissione, e si lega anche alla National Competent Authority.
![[Pasted image 20240607174639.png]]
![[Pasted image 20240607174502.png]]
![[Pasted image 20240607175021.png]]