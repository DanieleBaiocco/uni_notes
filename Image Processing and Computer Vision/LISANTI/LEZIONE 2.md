## Image Digitization
Nell'image plane c'e' un *planar sensor* che converte la luce in ogni punto in electric quantity.
**Come faccio A DISCRETIZZARE UNA IMMAGINE CONTINUA?**
CI sono 2 STEPS: 
1. **SAMPLING**: c'e' uno spatial sampling  dai FOTORECEPTORS che mappano i raggi in ELETTRICAL POINTS a una grid di dimension N x M. Praticamente in questo step si definisce la grid N x M, **SPECIFICANDO** il numero di PIXELS che voglio per la mia immagine.
2. **QUANTIZATION**: Questi devono POI essere convertiti in un valore che sia comprensibile dal PC. C'e' un QUANTIZATION STEP, in cui il valore                                                                                                                                                                                                                                                                    elettrico viene mappato in un numero che ne rappresenta L'INTENSITA', sotto forma di *gray levels*. Devo in QUANTIZATION scegliere quanti VALORI DI INTENSITA' voglio rappresentare. Questo dipende da QUANTI BITS voglio usare.
   Se ho _m_ come numero di bits scelti per rappresentare un pixel allora avro' $l = 2^m$ possibili valori discreti da associare a un pixel.
**NOTA** il numero totale di bits che servono per immagazzinare tutta l'informazione di un'immagine e' $B = N * M * m$ 
Per avere un'immagine colorata, ho bisogno di 3 channels. Per ogni channel avro' una matrice di pixels. 

## RELAZIONE TRA DIGITALIZATION E IMAGE QUALITY
Ovviamente **piu' PIXELS ho per digitalizzare l'immagine piu' questa risultera' definita**. Lo stesso vale per il **numero di bits** in ogni pixel, **piu' e' alto piu' intensita' di grigio posso rappresentare**.
Quando faccio una COARSER SAMPLING perdo MOLTI DETAILS, perdo subito quelli.
Se invece uso lo stesso numero di PIXELS se faccio un COARSER QUANTIZATION, quindi uso solo 4 bits di INTENSITA'. Ci sono in risposta molte meno shapes of gray. Entrambe le cose sono notabili dall'immagine qua sotto.
![[CV16.png]]
# Camera sensors
Come gia' detto ci sono **sensori** nell'image plane che convertono i fotoni (intensita' di luce) in elettroni (valori elettrici, legati al voltaggio). 
I sensori piu' usati sono:
1. **CCD** : ha solo un elettronic circuit che computa il valore elettrico per TUTTI i pixel. Viene utilizzato come MEDICAL EQUIPMENT.
2. **CMOS** :  ha un elettrnoic circuit  PER OGNI PIXEL. Questo mi permette di lavorare **SOLO SU UNA PARTE DELL'IMMAGIN**E. Nel caso di sopra devo PROCESSARE TUTTO SEMPRE. La maggior parte degli smartphones sono basati su CMOS
CCD e CMODS sono in grado di acquisire dei segnali nello spettro che L'UOMO non puo' vedere. Di sicuro non possono acquisire immagini colorate,**loro capiscono solo QUANTA INTENSITA' elettrica c'e'**.
**Come fare allora a registrare i colori?** C'e' da creare un *color sensor*, che e' un array di *optical filters* che viene messo DAVANTI ai *photodetectors* (che sono i sensori insomma), in modo da collezionare solo l'intensita' di luce proveniente SOLO da un determinato range di lunghezze d'onda (il range associato a un determinato colore).
Solitamente i filtri verdi sono il doppio di quelli rossi e blu, perche' l'occhio umano e' piu' sensibile al colore verde.

# Signal to Noise Ratio
Se guardo un immagine e il tempo passa, IN UN SOLO PIXEL NON OSSERVERO' MAI LO STESSO VALORE. Questo e' dovuto al fatto che c'e' sempre un PO' DI NOISE nell'acquisizione della luce.
Il **SIGNAL-to-NOISE-RATIO'** e' un valore che mi dice quanto il MIO SENSORE E' bravo. Misura quanto e' bravo il mio sensore nell'acquisire la LUCE anche considerando che ci sara' sempre del NOISE rispetto al REAL WORLD.

Le sources del noise sono :
1. **Photon Shot Noise**: The time between photon arrivals at a pixel is governed by a Poisson statistic and thus the number of photons collected during exposure time is not constant.
2. **Electronic Circuitry Noise**: It is generated by the electronics which reads-out the charge and amplifies the resulting voltage signal.
3. **Quantization Noise**: related to the final ADC conversion (in digital cameras).
4. **Dark Current Noise**: – a random amount of charge due to thermal excitement is observed at each pixel even though the sensor is not exposed to light. Di base c'e' sempre questa roba. Tipo quando oscuro la telecamera e la copro, non ho mai un'immagine completamente nera. Ho delle fluttuazioni dovute a questo motivo.
The **SNR** can be thought of as quantifying the strength of the “true” signal with respect to the unwanted fluctuations induced by noise (i.e. the higher the better).


# Dynamic Range
E' un altro valore di quanto il **sensore** e' bravo (nota che e' a livello del sensore).
Se nella scena ho **zone scure e zone chiare** saro' in grado di osservare dettagli **SOLO** in una delle due zone. Difficilmente avro' un *dynamic range* che mi permetta di avere dettagli per entrambi i contesti.
Cio' perche' NON ho abbastanza livelli **per rappresentare questo vastao range di valori** a livello del sensore. 
Piu' livelli ho piu' son bravo a rappresentarli. La formula e' la seguente:
$$DR = E_{max}/E_{min}$$
$E_{min}$ e' il *minimum detectable irradiation*, che dipende dalla COMPONENTE DI NOISE DISCUSSA PRIMA. E' il minimo che posso detectare dato il fatto che ci sia il noise.
$E_{max}$ e ' il *valore di saturation* oltre il quale io non posso detectare piu' nulla.
Voglio che _DR_ sia piu' alto possibile.

Volgio l'HIGHEST DYNAMIC RANGE per vedere dettagli della parte piu' oscura dell'immagine e quella piu' bright nella stessa immagine. CAPISCI? non ho abbastanza valori
Come soluzione creo due immagini: una in cui si e' bravi con la parte bright e una in cui si e' bravi con la parte dark, e poi LE COMBINO.

# DENOISING
So che ogni immagine che acquisisco e' soggetta a _NOISE_ per tutti i motivi gia' visti. 
Assumo di far foto a una **scena statica**. Aquisisco 100 immagini da questa scena.
Queste isulteranno **TUTTE DIFFERENTI** (non saranno mai IDENTICHE a livello di pixel, in quanto ci sara' sempre del noise in punti diversi). 
C'e' un assunzione che viene fatta riguardo al _noise_ nelle immagini:
1. E' **indipendente**: il _noise_ generato da un pixel non dipende dal _noise_ generato da un altro pixel
2. E' **identically distributed**: i valori del noise seguono quindi una distribuzione gaussiana con _mean_ a 0 e _standard deviation_ a noi ignota. 
Ho questa formula per il noise quindi:
![[CV17.png]]
Con _p_ pixel, _$I_k(p)$_ valore registrato in quel pixel dalla camera, $\hat{I}(p)$ valore ideale (privo di noise) per quel pixel _p_, $n_k(p)$ il noise value della _k-esima_ immagine scattata sul pixel _p_. 
**COME LIMITO QUESTO NOISE? COME LO RIMUOVO?**
## INTEGRATING OVER TIME
**NOTA**: ESSENDO CHE il noise E' samplato DA UNA NORMAL DISTRIBUTION CON 0 MEAN, se colleziono MOLTE IMMAGINI posso FARE la media dei vari valori di noise registrati in ogni immagine.
Collezionando piu' immagini approssimo la gaussian distribution da cui il noise e' stato samplato, e facendo **averaging** tolgo il noise (secondo l'assunzione iniziale sulla zero mean del noise, otterrei noise = 0). Questa formula mostra come posso fare denoising:
![[CV18.png]]
Questo approccio funziona **SOLO SU STATIC IMAGES**.
Ecco i risultati:
 ![[CV19 1.png]]
## INTEGRATING OVER SPACE
Se ho un PIXEL e voglio sapere l'IDEAL NOISELESS VALUE per quel pixel, invece di integrare over time INTEGRO A LIVELLO SPAZIALE (dai pixel che sono neighborhoods).
Definisco una _supporting window K_ intorno a _p_. Prendo tutti i _q_ in _K_ e faccio _l'average di questi pixels_. In sto modo faccio _denoising_ in un altro modo.  Questo dovrebbe **ridurre** il NOISE.  **Nota in K c'e' anche il punto p**.
La formula e' la seguente:
![[CV20.png]]
**Quanto deve essere grande K?**
E' un TRADEOFF. 

Questo funziona se nella window INTORNO ho valori UGUALI o comunque legati allo stesso oggetto. **NON puo' funzionare** se ci sono **ALTRI DETTAGLI DI ALTRI OGGETTI nella K**.
Per questo dicevo che era un tradeoff: se aumento K diminuisco il noise ma rischio di inglobare anche  altri oggetti nella K, e i valori dei loro pixels verranno (a malincuore) usati quando si fara' la mean.
**NOTA**: L'**INTEGRATION OVER SPACE** non necessita di obbligatoriamente *static images*. Infatti questo approccio e' applicabile solo con una foto (non serve scattare tante foto ferme come prima).

